{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dfdf78-932f-45de-a762-0b6223b84b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafe4ddd-196c-450f-8a54-cbb588bbec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827635 sha256=5fe3e4c6d75241f9a05799fc010d63f54b2a89873633975c05dccf63574d52d8\n",
      "  Stored in directory: c:\\users\\19096\\appdata\\local\\pip\\cache\\wheels\\95\\51\\6c\\9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f59b7b-1fc4-44cc-860e-a2963e73958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 6.8/38.8 MB 46.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.1/38.8 MB 47.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.8 MB 52.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.8 MB 51.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 45.7 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc80a64c-1bd7-4f45-a060-a87e09f1610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f96b5c-7f64-4981-920a-96eca2a2da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.9.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
      "Requirement already satisfied: pygame in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.66.5)\n",
      "Requirement already satisfied: rich in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.7.1)\n",
      "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
      "  Downloading ale_py-0.10.1-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Downloading ale_py-0.10.1-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.6/4.4 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: tensorboard-data-server, grpcio, ale-py, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 ale-py-0.10.1 grpcio-1.68.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4dc03d18-cf8e-40ea-818e-76d67ea0f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77333010-2de0-4148-8f9d-a6593aecf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e389fd9-49be-4264-a3e2-738a0ba0c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92b6f39-758e-43db-9d04-465964fed079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "410706f0-ca56-4457-9664-ac49e52c8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym1(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        #Sets up observation space(image of game) and action space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 2)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized  = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.reshape(resized, (1, 100, 160))\n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bcec28-d804-41d7-a91f-981aba4743ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym2(Env):\n",
    "    def __init__(self, render=False, config='github/VizDoom/scenarios/defend_the_center.cfg'):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "        #self.ammo = 26\n",
    "        self.living_reward = 0.1\n",
    "\n",
    "        #Sets up observation space(image of game) and action space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        base_reward = self.game.make_action(actions[action], 2)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "\n",
    "        reward = base_reward + self.living_reward\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized  = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.reshape(resized, (1, 100, 160))\n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09277eb2-78b2-439e-8fd0-b43719b96706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, trials=5):\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits, _ = agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.argmax(probs, dim=-1).item()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                total_reward += reward\n",
    "                done = terminated\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "        average_reward = np.mean(total_rewards)\n",
    "        print(f\"Average Reward over {trials} episodes: {average_reward}\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bbb3a74-fbdb-4c4c-a492-17ca6d0413be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #based logic on stablebaselines\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),  # Output: [32, 24, 39]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: [64, 11, 18]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: [64, 9, 16]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(512, action_space.n)\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        x = self.forward(state)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f88e0f4e-b3f7-486d-8f1b-220d7c4b64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, agent, run_name, epochs=10, clip_eps=0.2, gamma=0.99, lr=1e-4, batch_size=3000):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.clip_eps = clip_eps\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "        log_dir = os.path.join(\"ppo_logs\", run_name)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0  \n",
    "\n",
    "    def compute_returns(self, rewards, dones, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            if dones[step]:\n",
    "                R = 0  \n",
    "            R = rewards[step] + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        timesteps = 0\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0\n",
    "        episode = 0\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            states = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            dones = []\n",
    "\n",
    "\n",
    "            for _ in range(self.batch_size):\n",
    "                logits, value = self.agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob.detach())\n",
    "                rewards.append(reward)\n",
    "                values.append(value.detach().item())\n",
    "                dones.append(terminated)\n",
    "\n",
    "                episode_reward += reward\n",
    "                self.global_step += 1  \n",
    "\n",
    "                if terminated:\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "\n",
    "                    # Calculate and log average reward\n",
    "                    if len(episode_rewards) >= 100:\n",
    "                        average_reward = np.mean(episode_rewards[-100:])\n",
    "                    else:\n",
    "                        average_reward = np.mean(episode_rewards)\n",
    "                    self.writer.add_scalar(\"Average Reward\", average_reward, self.global_step)\n",
    "\n",
    "                    episode_reward = 0\n",
    "                    episode += 1\n",
    "                    state, _ = self.env.reset()\n",
    "                    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "                timesteps += 1\n",
    "\n",
    "            returns = self.compute_returns(rewards, dones, self.gamma)\n",
    "\n",
    "            states = torch.cat(states)\n",
    "            actions = torch.tensor(actions).unsqueeze(-1)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            returns = torch.tensor(returns).unsqueeze(-1)\n",
    "            values = torch.tensor(values).unsqueeze(-1)\n",
    "\n",
    "            advantages = returns - values\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            losses = []\n",
    "            entropies = []\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                logits, value = self.agent.get_action(states)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = (new_log_probs - log_probs).exp()\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
    "\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.MSELoss()(value, returns)\n",
    "                loss = 100*policy_loss + 0.5 * value_loss - 0.03 * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "            \n",
    "            avg_policy_loss = np.mean(policy_losses)\n",
    "            avg_value_loss = np.mean(value_losses)\n",
    "            avg_entropy = np.mean(entropies)\n",
    "            avg_loss = np.mean(losses)\n",
    "\n",
    "            self.writer.add_scalar(\"Loss\", avg_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Policy Loss\", avg_policy_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Value Loss\", avg_value_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Entropy\", avg_entropy, self.global_step)\n",
    "\n",
    "            print(f\"Timesteps: {timesteps}, Policy Loss: {avg_policy_loss}, \"f\"Value Loss: {avg_value_loss}, Entropy: {avg_entropy}, \"f\"Combined Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ded7483-afe4-4d6a-b92b-f9a9151129fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 12.09999999999997\n",
      "Episode 1 Reward: 20.20000000000004\n",
      "Episode 2 Reward: 12.09999999999997\n",
      "Episode 3 Reward: 11.199999999999973\n",
      "Episode 4 Reward: 12.09999999999997\n",
      "Episode 5 Reward: 15.999999999999973\n",
      "Episode 6 Reward: 12.09999999999997\n",
      "Episode 7 Reward: 20.90000000000003\n",
      "Episode 8 Reward: 16.69999999999998\n",
      "Episode 9 Reward: 16.799999999999986\n",
      "Episode 10 Reward: 15.699999999999966\n",
      "Episode 11 Reward: 17.69999999999998\n",
      "Episode 12 Reward: 14.59999999999996\n",
      "Episode 13 Reward: 17.199999999999996\n",
      "Episode 14 Reward: 14.799999999999963\n",
      "Episode 15 Reward: 16.199999999999978\n",
      "Episode 16 Reward: 19.20000000000002\n",
      "Episode 17 Reward: 13.799999999999967\n",
      "Episode 18 Reward: 11.599999999999971\n",
      "Timesteps: 3000, Policy Loss: -0.00046517623418509404, Value Loss: 25.249364852905273, Entropy: 1.0977853417396546, Combined Loss: 12.545231246948243\n",
      "Episode 19 Reward: 10.899999999999977\n",
      "Episode 20 Reward: 16.199999999999978\n",
      "Episode 21 Reward: 11.79999999999997\n",
      "Episode 22 Reward: 12.09999999999997\n",
      "Episode 23 Reward: 16.199999999999978\n",
      "Episode 24 Reward: 16.199999999999978\n",
      "Episode 25 Reward: 17.09999999999999\n",
      "Episode 26 Reward: 19.600000000000026\n",
      "Episode 27 Reward: 12.899999999999968\n",
      "Episode 28 Reward: 19.700000000000028\n",
      "Episode 29 Reward: 16.89999999999999\n",
      "Episode 30 Reward: 16.199999999999978\n",
      "Episode 31 Reward: 21.700000000000028\n",
      "Episode 32 Reward: 22.900000000000063\n",
      "Episode 33 Reward: 16.199999999999978\n",
      "Episode 34 Reward: 12.899999999999967\n",
      "Episode 35 Reward: 20.100000000000037\n",
      "Episode 36 Reward: 9.399999999999979\n",
      "Episode 37 Reward: 23.10000000000008\n",
      "Timesteps: 6000, Policy Loss: -0.0001216901468676923, Value Loss: 14.035647296905518, Entropy: 1.0910703182220458, Combined Loss: 6.972922277450562\n",
      "Episode 38 Reward: 12.899999999999968\n",
      "Episode 39 Reward: 16.199999999999978\n",
      "Episode 40 Reward: 15.399999999999968\n",
      "Episode 41 Reward: 24.300000000000082\n",
      "Episode 42 Reward: 13.899999999999967\n",
      "Episode 43 Reward: 15.599999999999968\n",
      "Episode 44 Reward: 12.09999999999997\n",
      "Episode 45 Reward: 12.09999999999997\n",
      "Episode 46 Reward: 18.000000000000004\n",
      "Episode 47 Reward: 17.199999999999992\n",
      "Episode 48 Reward: 9.599999999999978\n",
      "Episode 49 Reward: 19.900000000000034\n",
      "Episode 50 Reward: 15.399999999999961\n",
      "Episode 51 Reward: 16.199999999999978\n",
      "Episode 52 Reward: 20.700000000000028\n",
      "Episode 53 Reward: 14.299999999999965\n",
      "Episode 54 Reward: 16.199999999999978\n",
      "Episode 55 Reward: 16.199999999999978\n",
      "Timesteps: 9000, Policy Loss: 0.00017558149256120538, Value Loss: 9.252923011779785, Entropy: 1.0758082389831543, Combined Loss: 4.611745357513428\n",
      "Episode 56 Reward: 24.100000000000076\n",
      "Episode 57 Reward: 23.300000000000082\n",
      "Episode 58 Reward: 12.09999999999997\n",
      "Episode 59 Reward: 15.999999999999973\n",
      "Episode 60 Reward: 12.09999999999997\n",
      "Episode 61 Reward: 19.600000000000012\n",
      "Episode 62 Reward: 16.899999999999988\n",
      "Episode 63 Reward: 21.100000000000037\n",
      "Episode 64 Reward: 27.700000000000117\n",
      "Episode 65 Reward: 18.600000000000012\n",
      "Episode 66 Reward: 27.100000000000097\n",
      "Episode 67 Reward: 16.199999999999978\n",
      "Episode 68 Reward: 17.299999999999997\n",
      "Episode 69 Reward: 12.09999999999997\n",
      "Episode 70 Reward: 12.09999999999997\n",
      "Episode 71 Reward: 12.09999999999997\n",
      "Episode 72 Reward: 16.199999999999978\n",
      "Episode 73 Reward: 18.50000000000001\n",
      "Timesteps: 12000, Policy Loss: -0.0033006024655119147, Value Loss: 9.886646270751953, Entropy: 1.067905604839325, Combined Loss: 4.5812256813049315\n",
      "Episode 74 Reward: 18.800000000000015\n",
      "Episode 75 Reward: 21.700000000000028\n",
      "Episode 76 Reward: 25.500000000000085\n",
      "Episode 77 Reward: 20.700000000000028\n",
      "Episode 78 Reward: 25.100000000000108\n",
      "Episode 79 Reward: 22.30000000000006\n",
      "Episode 80 Reward: 16.29999999999998\n",
      "Episode 81 Reward: 18.500000000000014\n",
      "Episode 82 Reward: 15.599999999999968\n",
      "Episode 83 Reward: 13.09999999999997\n",
      "Episode 84 Reward: 16.29999999999998\n",
      "Episode 85 Reward: 16.199999999999978\n",
      "Episode 86 Reward: 23.700000000000056\n",
      "Episode 87 Reward: 24.10000000000008\n",
      "Episode 88 Reward: 26.500000000000103\n",
      "Episode 89 Reward: 25.500000000000085\n",
      "Timesteps: 15000, Policy Loss: 0.0003401273424969986, Value Loss: 10.693711948394775, Entropy: 1.0454576134681701, Combined Loss: 5.349504995346069\n",
      "Episode 90 Reward: 16.199999999999978\n",
      "Episode 91 Reward: 15.799999999999974\n",
      "Episode 92 Reward: 16.199999999999978\n",
      "Episode 93 Reward: 10.899999999999974\n",
      "Episode 94 Reward: 16.199999999999978\n",
      "Episode 95 Reward: 27.10000000000009\n",
      "Episode 96 Reward: 20.400000000000023\n",
      "Episode 97 Reward: 16.199999999999978\n",
      "Episode 98 Reward: 16.199999999999978\n",
      "Episode 99 Reward: 17.299999999999997\n",
      "Episode 100 Reward: 19.10000000000002\n",
      "Episode 101 Reward: 22.900000000000063\n",
      "Episode 102 Reward: 20.700000000000028\n",
      "Episode 103 Reward: 35.10000000000022\n",
      "Episode 104 Reward: 16.899999999999984\n",
      "Episode 105 Reward: 14.299999999999965\n",
      "Timesteps: 18000, Policy Loss: -0.00019315246910949212, Value Loss: 9.922057628631592, Entropy: 1.0703802227973938, Combined Loss: 4.909602212905884\n",
      "Episode 106 Reward: 17.8\n",
      "Episode 107 Reward: 21.700000000000028\n",
      "Episode 108 Reward: 29.600000000000136\n",
      "Episode 109 Reward: 25.100000000000108\n",
      "Episode 110 Reward: 25.90000000000009\n",
      "Episode 111 Reward: 25.500000000000085\n",
      "Episode 112 Reward: 17.09999999999999\n",
      "Episode 113 Reward: 22.900000000000077\n",
      "Episode 114 Reward: 25.90000000000008\n",
      "Episode 115 Reward: 23.700000000000077\n",
      "Episode 116 Reward: 16.499999999999982\n",
      "Episode 117 Reward: 16.199999999999978\n",
      "Episode 118 Reward: 23.500000000000075\n",
      "Episode 119 Reward: 14.299999999999965\n",
      "Episode 120 Reward: 18.700000000000014\n",
      "Timesteps: 21000, Policy Loss: -0.0023318539535005733, Value Loss: 10.714782524108887, Entropy: 0.982407420873642, Combined Loss: 5.094733715057373\n",
      "Episode 121 Reward: 24.300000000000082\n",
      "Episode 122 Reward: 22.300000000000068\n",
      "Episode 123 Reward: 23.300000000000068\n",
      "Episode 124 Reward: 17.4\n",
      "Episode 125 Reward: 23.500000000000057\n",
      "Episode 126 Reward: 27.90000000000012\n",
      "Episode 127 Reward: 24.10000000000008\n",
      "Episode 128 Reward: 24.600000000000072\n",
      "Episode 129 Reward: 22.40000000000007\n",
      "Episode 130 Reward: 17.4\n",
      "Episode 131 Reward: 15.199999999999962\n",
      "Episode 132 Reward: 30.900000000000148\n",
      "Episode 133 Reward: 17.4\n",
      "Episode 134 Reward: 21.700000000000045\n",
      "Episode 135 Reward: 26.500000000000085\n",
      "Timesteps: 24000, Policy Loss: -0.0011402318124620336, Value Loss: 10.528505992889404, Entropy: 0.9930624723434448, Combined Loss: 5.120438003540039\n",
      "Episode 136 Reward: 17.4\n",
      "Episode 137 Reward: 23.900000000000077\n",
      "Episode 138 Reward: 36.700000000000216\n",
      "Episode 139 Reward: 21.70000000000006\n",
      "Episode 140 Reward: 17.7\n",
      "Episode 141 Reward: 25.500000000000085\n",
      "Episode 142 Reward: 33.900000000000176\n",
      "Episode 143 Reward: 16.39999999999998\n",
      "Episode 144 Reward: 30.20000000000017\n",
      "Episode 145 Reward: 23.70000000000006\n",
      "Episode 146 Reward: 23.500000000000068\n",
      "Episode 147 Reward: 21.70000000000003\n",
      "Episode 148 Reward: 25.300000000000082\n",
      "Episode 149 Reward: 21.30000000000004\n",
      "Timesteps: 27000, Policy Loss: -8.774954491856946e-05, Value Loss: 11.726707458496094, Entropy: 1.0486577153205872, Combined Loss: 5.82311897277832\n",
      "Episode 150 Reward: 24.700000000000085\n",
      "Episode 151 Reward: 23.900000000000077\n",
      "Episode 152 Reward: 31.70000000000016\n",
      "Episode 153 Reward: 31.100000000000136\n",
      "Episode 154 Reward: 12.09999999999997\n",
      "Episode 155 Reward: 15.49999999999997\n",
      "Episode 156 Reward: 21.10000000000005\n",
      "Episode 157 Reward: 28.700000000000117\n",
      "Episode 158 Reward: 16.199999999999978\n",
      "Episode 159 Reward: 24.300000000000068\n",
      "Episode 160 Reward: 16.199999999999978\n",
      "Episode 161 Reward: 19.100000000000005\n",
      "Episode 162 Reward: 21.10000000000005\n",
      "Episode 163 Reward: 16.39999999999998\n",
      "Episode 164 Reward: 16.199999999999978\n",
      "Timesteps: 30000, Policy Loss: -0.001587624962944112, Value Loss: 10.679139423370362, Entropy: 1.0471283435821532, Combined Loss: 5.149393320083618\n",
      "Episode 165 Reward: 23.700000000000074\n",
      "Episode 166 Reward: 32.100000000000186\n",
      "Episode 167 Reward: 20.10000000000002\n",
      "Episode 168 Reward: 24.10000000000008\n",
      "Episode 169 Reward: 25.300000000000082\n",
      "Episode 170 Reward: 30.100000000000154\n",
      "Episode 171 Reward: 23.300000000000068\n",
      "Episode 172 Reward: 20.700000000000045\n",
      "Episode 173 Reward: 22.900000000000063\n",
      "Episode 174 Reward: 23.50000000000007\n",
      "Episode 175 Reward: 16.39999999999998\n",
      "Episode 176 Reward: 26.300000000000107\n",
      "Episode 177 Reward: 12.09999999999997\n",
      "Episode 178 Reward: 21.300000000000054\n",
      "Timesteps: 33000, Policy Loss: -0.0019454358478697919, Value Loss: 11.010839653015136, Entropy: 1.0474137663841248, Combined Loss: 5.279453754425049\n",
      "Episode 179 Reward: 24.20000000000008\n",
      "Episode 180 Reward: 21.300000000000054\n",
      "Episode 181 Reward: 23.300000000000068\n",
      "Episode 182 Reward: 24.50000000000009\n",
      "Episode 183 Reward: 30.300000000000143\n",
      "Episode 184 Reward: 20.700000000000045\n",
      "Episode 185 Reward: 25.900000000000105\n",
      "Episode 186 Reward: 23.300000000000068\n",
      "Episode 187 Reward: 28.40000000000014\n",
      "Episode 188 Reward: 30.100000000000122\n",
      "Episode 189 Reward: 35.10000000000019\n",
      "Episode 190 Reward: 24.700000000000088\n",
      "Episode 191 Reward: 12.09999999999997\n",
      "Episode 192 Reward: 18.90000000000002\n",
      "Timesteps: 36000, Policy Loss: -0.002585449360730152, Value Loss: 11.557810020446777, Entropy: 1.002816516160965, Combined Loss: 5.490275526046753\n",
      "Episode 193 Reward: 24.500000000000085\n",
      "Episode 194 Reward: 26.400000000000098\n",
      "Episode 195 Reward: 25.900000000000077\n",
      "Episode 196 Reward: 29.300000000000125\n",
      "Episode 197 Reward: 36.000000000000206\n",
      "Episode 198 Reward: 20.300000000000022\n",
      "Episode 199 Reward: 20.90000000000005\n",
      "Episode 200 Reward: 21.700000000000045\n",
      "Episode 201 Reward: 22.50000000000007\n",
      "Episode 202 Reward: 23.900000000000077\n",
      "Episode 203 Reward: 17.4\n",
      "Episode 204 Reward: 25.400000000000084\n",
      "Episode 205 Reward: 17.600000000000005\n",
      "Episode 206 Reward: 20.10000000000002\n",
      "Timesteps: 39000, Policy Loss: 0.0018191213529867411, Value Loss: 10.501397895812989, Entropy: 1.0307462632656097, Combined Loss: 5.401688718795777\n",
      "Episode 207 Reward: 22.300000000000068\n",
      "Episode 208 Reward: 24.300000000000068\n",
      "Episode 209 Reward: 21.300000000000036\n",
      "Episode 210 Reward: 36.20000000000022\n",
      "Episode 211 Reward: 24.900000000000077\n",
      "Episode 212 Reward: 23.900000000000077\n",
      "Episode 213 Reward: 24.600000000000087\n",
      "Episode 214 Reward: 20.300000000000022\n",
      "Episode 215 Reward: 26.500000000000103\n",
      "Episode 216 Reward: 23.30000000000005\n",
      "Episode 217 Reward: 16.39999999999998\n",
      "Episode 218 Reward: 25.700000000000102\n",
      "Episode 219 Reward: 24.500000000000068\n",
      "Timesteps: 42000, Policy Loss: -0.0005459487035395227, Value Loss: 10.22797508239746, Entropy: 0.9819399297237397, Combined Loss: 5.029934501647949\n",
      "Episode 220 Reward: 27.70000000000012\n",
      "Episode 221 Reward: 18.09999999999999\n",
      "Episode 222 Reward: 21.10000000000002\n",
      "Episode 223 Reward: 25.900000000000105\n",
      "Episode 224 Reward: 17.7\n",
      "Episode 225 Reward: 28.50000000000013\n",
      "Episode 226 Reward: 21.900000000000055\n",
      "Episode 227 Reward: 28.400000000000112\n",
      "Episode 228 Reward: 19.600000000000012\n",
      "Episode 229 Reward: 16.199999999999978\n",
      "Episode 230 Reward: 24.50000000000009\n",
      "Episode 231 Reward: 20.80000000000003\n",
      "Episode 232 Reward: 14.899999999999963\n",
      "Episode 233 Reward: 25.500000000000117\n",
      "Episode 234 Reward: 10.899999999999974\n",
      "Episode 235 Reward: 24.900000000000095\n",
      "Timesteps: 45000, Policy Loss: -0.004020473312741757, Value Loss: 10.567483520507812, Entropy: 1.004034733772278, Combined Loss: 4.851573419570923\n",
      "Episode 236 Reward: 26.100000000000094\n",
      "Episode 237 Reward: 28.100000000000108\n",
      "Episode 238 Reward: 25.900000000000105\n",
      "Episode 239 Reward: 23.300000000000054\n",
      "Episode 240 Reward: 17.099999999999994\n",
      "Episode 241 Reward: 24.400000000000073\n",
      "Episode 242 Reward: 21.70000000000006\n",
      "Episode 243 Reward: 26.500000000000103\n",
      "Episode 244 Reward: 23.700000000000074\n",
      "Episode 245 Reward: 19.400000000000013\n",
      "Episode 246 Reward: 23.100000000000065\n",
      "Episode 247 Reward: 26.10000000000011\n",
      "Episode 248 Reward: 23.10000000000008\n",
      "Episode 249 Reward: 29.500000000000114\n",
      "Timesteps: 48000, Policy Loss: 0.00025511371595587917, Value Loss: 11.270083045959472, Entropy: 0.9812230885028839, Combined Loss: 5.631116247177124\n",
      "Episode 250 Reward: 18.4\n",
      "Episode 251 Reward: 23.700000000000088\n",
      "Episode 252 Reward: 27.500000000000114\n",
      "Episode 253 Reward: 25.90000000000009\n",
      "Episode 254 Reward: 24.900000000000105\n",
      "Episode 255 Reward: 16.39999999999998\n",
      "Episode 256 Reward: 24.200000000000085\n",
      "Episode 257 Reward: 21.900000000000045\n",
      "Episode 258 Reward: 24.300000000000082\n",
      "Episode 259 Reward: 22.10000000000005\n",
      "Episode 260 Reward: 26.300000000000097\n",
      "Episode 261 Reward: 17.4\n",
      "Episode 262 Reward: 28.300000000000118\n",
      "Episode 263 Reward: 22.100000000000065\n",
      "Timesteps: 51000, Policy Loss: -0.001992124831986919, Value Loss: 10.554512023925781, Entropy: 0.9857804477214813, Combined Loss: 5.0484702587127686\n",
      "Episode 264 Reward: 15.599999999999968\n",
      "Episode 265 Reward: 30.500000000000128\n",
      "Episode 266 Reward: 24.800000000000093\n",
      "Episode 267 Reward: 21.90000000000005\n",
      "Episode 268 Reward: 27.500000000000114\n",
      "Episode 269 Reward: 16.89999999999999\n",
      "Episode 270 Reward: 24.500000000000068\n",
      "Episode 271 Reward: 37.50000000000024\n",
      "Episode 272 Reward: 15.899999999999972\n",
      "Episode 273 Reward: 27.30000000000011\n",
      "Episode 274 Reward: 14.199999999999962\n",
      "Episode 275 Reward: 24.100000000000083\n",
      "Episode 276 Reward: 17.7\n",
      "Episode 277 Reward: 18.100000000000005\n",
      "Timesteps: 54000, Policy Loss: -0.002408800673071454, Value Loss: 11.691442203521728, Entropy: 0.9518251121044159, Combined Loss: 5.576286315917969\n",
      "Episode 278 Reward: 27.100000000000115\n",
      "Episode 279 Reward: 14.199999999999969\n",
      "Episode 280 Reward: 24.700000000000074\n",
      "Episode 281 Reward: 24.10000000000008\n",
      "Episode 282 Reward: 25.700000000000102\n",
      "Episode 283 Reward: 27.30000000000011\n",
      "Episode 284 Reward: 18.200000000000014\n",
      "Episode 285 Reward: 41.00000000000026\n",
      "Episode 286 Reward: 35.8000000000002\n",
      "Episode 287 Reward: 27.300000000000114\n",
      "Episode 288 Reward: 25.90000000000009\n",
      "Episode 289 Reward: 23.300000000000058\n",
      "Episode 290 Reward: 20.90000000000005\n",
      "Episode 291 Reward: 25.50000000000007\n",
      "Timesteps: 57000, Policy Loss: -0.0013353343257564098, Value Loss: 12.305084419250488, Entropy: 0.9177010834217072, Combined Loss: 5.991477727890015\n",
      "Episode 292 Reward: 24.10000000000007\n",
      "Episode 293 Reward: 16.99999999999997\n",
      "Episode 294 Reward: 27.500000000000117\n",
      "Episode 295 Reward: 25.10000000000011\n",
      "Episode 296 Reward: 18.299999999999994\n",
      "Episode 297 Reward: 22.100000000000037\n",
      "Episode 298 Reward: 11.199999999999973\n",
      "Episode 299 Reward: 19.500000000000018\n",
      "Episode 300 Reward: 38.800000000000246\n",
      "Episode 301 Reward: 34.70000000000019\n",
      "Episode 302 Reward: 17.39999999999998\n",
      "Episode 303 Reward: 39.30000000000025\n",
      "Episode 304 Reward: 37.40000000000024\n",
      "Timesteps: 60000, Policy Loss: -0.0022712887659824153, Value Loss: 12.565155410766602, Entropy: 0.9178334712982178, Combined Loss: 6.027913761138916\n",
      "Episode 305 Reward: 13.09999999999997\n",
      "Episode 306 Reward: 23.700000000000056\n",
      "Episode 307 Reward: 27.400000000000116\n",
      "Episode 308 Reward: 19.100000000000005\n",
      "Episode 309 Reward: 22.70000000000006\n",
      "Episode 310 Reward: 24.100000000000065\n",
      "Episode 311 Reward: 28.30000000000011\n",
      "Episode 312 Reward: 23.100000000000065\n",
      "Episode 313 Reward: 21.900000000000066\n",
      "Episode 314 Reward: 13.099999999999966\n",
      "Episode 315 Reward: 31.80000000000016\n",
      "Episode 316 Reward: 21.900000000000038\n",
      "Episode 317 Reward: 20.000000000000014\n",
      "Episode 318 Reward: 23.50000000000007\n",
      "Episode 319 Reward: 23.600000000000072\n",
      "Timesteps: 63000, Policy Loss: -0.0018515802884474298, Value Loss: 9.632330989837646, Entropy: 0.9344781994819641, Combined Loss: 4.6029730319976805\n",
      "Episode 320 Reward: 13.399999999999968\n",
      "Episode 321 Reward: 27.500000000000114\n",
      "Episode 322 Reward: 38.80000000000026\n",
      "Episode 323 Reward: 21.10000000000005\n",
      "Episode 324 Reward: 14.099999999999966\n",
      "Episode 325 Reward: 31.500000000000142\n",
      "Episode 326 Reward: 26.900000000000123\n",
      "Episode 327 Reward: 28.900000000000134\n",
      "Episode 328 Reward: 16.39999999999998\n",
      "Episode 329 Reward: 17.4\n",
      "Episode 330 Reward: 28.50000000000013\n",
      "Episode 331 Reward: 21.50000000000004\n",
      "Episode 332 Reward: 21.70000000000006\n",
      "Episode 333 Reward: 17.7\n",
      "Timesteps: 66000, Policy Loss: -0.0005002725094508164, Value Loss: 11.738058185577392, Entropy: 0.9524093329906463, Combined Loss: 5.790429544448853\n",
      "Episode 334 Reward: 26.800000000000118\n",
      "Episode 335 Reward: 26.200000000000095\n",
      "Episode 336 Reward: 25.300000000000097\n",
      "Episode 337 Reward: 24.300000000000082\n",
      "Episode 338 Reward: 21.900000000000063\n",
      "Episode 339 Reward: 33.60000000000017\n",
      "Episode 340 Reward: 23.300000000000068\n",
      "Episode 341 Reward: 25.700000000000102\n",
      "Episode 342 Reward: 17.4\n",
      "Episode 343 Reward: 31.300000000000146\n",
      "Episode 344 Reward: 21.700000000000028\n",
      "Episode 345 Reward: 31.500000000000156\n",
      "Episode 346 Reward: 24.10000000000008\n",
      "Timesteps: 69000, Policy Loss: -0.0015752431262207266, Value Loss: 10.956966972351074, Entropy: 0.9987567484378814, Combined Loss: 5.290996503829956\n",
      "Episode 347 Reward: 17.599999999999998\n",
      "Episode 348 Reward: 25.300000000000097\n",
      "Episode 349 Reward: 24.000000000000064\n",
      "Episode 350 Reward: 20.700000000000028\n",
      "Episode 351 Reward: 25.5000000000001\n",
      "Episode 352 Reward: 24.500000000000085\n",
      "Episode 353 Reward: 20.400000000000027\n",
      "Episode 354 Reward: 19.100000000000005\n",
      "Episode 355 Reward: 23.10000000000005\n",
      "Episode 356 Reward: 22.700000000000045\n",
      "Episode 357 Reward: 33.000000000000206\n",
      "Episode 358 Reward: 26.900000000000105\n",
      "Episode 359 Reward: 27.5000000000001\n",
      "Episode 360 Reward: 33.70000000000017\n",
      "Timesteps: 72000, Policy Loss: -0.00017592833303350374, Value Loss: 11.003604412078857, Entropy: 1.0012117803096772, Combined Loss: 5.454173040390015\n",
      "Episode 361 Reward: 30.100000000000154\n",
      "Episode 362 Reward: 23.700000000000056\n",
      "Episode 363 Reward: 21.300000000000058\n",
      "Episode 364 Reward: 27.000000000000124\n",
      "Episode 365 Reward: 27.30000000000011\n",
      "Episode 366 Reward: 31.700000000000173\n",
      "Episode 367 Reward: 32.30000000000018\n",
      "Episode 368 Reward: 32.200000000000166\n",
      "Episode 369 Reward: 22.300000000000043\n",
      "Episode 370 Reward: 26.500000000000085\n",
      "Episode 371 Reward: 26.5000000000001\n",
      "Episode 372 Reward: 25.200000000000095\n",
      "Timesteps: 75000, Policy Loss: -0.0011184189672892586, Value Loss: 10.548478412628175, Entropy: 0.9554343104362488, Combined Loss: 5.13373441696167\n",
      "Episode 373 Reward: 30.900000000000134\n",
      "Episode 374 Reward: 24.300000000000068\n",
      "Episode 375 Reward: 24.50000000000007\n",
      "Episode 376 Reward: 29.500000000000128\n",
      "Episode 377 Reward: 25.500000000000103\n",
      "Episode 378 Reward: 33.10000000000018\n",
      "Episode 379 Reward: 15.599999999999971\n",
      "Episode 380 Reward: 32.100000000000136\n",
      "Episode 381 Reward: 24.70000000000009\n",
      "Episode 382 Reward: 16.39999999999998\n",
      "Episode 383 Reward: 26.7000000000001\n",
      "Episode 384 Reward: 30.500000000000146\n",
      "Episode 385 Reward: 25.700000000000085\n",
      "Timesteps: 78000, Policy Loss: -0.0017111145868392796, Value Loss: 11.034338665008544, Entropy: 0.9668121039867401, Combined Loss: 5.317053604125976\n",
      "Episode 386 Reward: 35.90000000000022\n",
      "Episode 387 Reward: 25.10000000000008\n",
      "Episode 388 Reward: 18.10000000000001\n",
      "Episode 389 Reward: 26.500000000000103\n",
      "Episode 390 Reward: 27.700000000000117\n",
      "Episode 391 Reward: 21.60000000000004\n",
      "Episode 392 Reward: 28.30000000000011\n",
      "Episode 393 Reward: 24.300000000000082\n",
      "Episode 394 Reward: 23.50000000000007\n",
      "Episode 395 Reward: 31.700000000000145\n",
      "Episode 396 Reward: 34.6000000000002\n",
      "Episode 397 Reward: 17.39999999999999\n",
      "Episode 398 Reward: 45.50000000000031\n",
      "Timesteps: 81000, Policy Loss: -0.0010173770486765043, Value Loss: 11.839110469818115, Entropy: 0.9691038429737091, Combined Loss: 5.788744211196899\n",
      "Episode 399 Reward: 16.39999999999998\n",
      "Episode 400 Reward: 22.900000000000063\n",
      "Episode 401 Reward: 22.700000000000028\n",
      "Episode 402 Reward: 16.39999999999998\n",
      "Episode 403 Reward: 30.10000000000014\n",
      "Episode 404 Reward: 31.500000000000156\n",
      "Episode 405 Reward: 33.5000000000002\n",
      "Episode 406 Reward: 27.400000000000126\n",
      "Episode 407 Reward: 23.10000000000007\n",
      "Episode 408 Reward: 20.90000000000005\n",
      "Episode 409 Reward: 24.50000000000007\n",
      "Episode 410 Reward: 16.39999999999998\n",
      "Episode 411 Reward: 22.300000000000068\n",
      "Episode 412 Reward: 24.300000000000082\n",
      "Timesteps: 84000, Policy Loss: -0.002754660396526276, Value Loss: 10.731189441680907, Entropy: 0.9556956946849823, Combined Loss: 5.061457920074463\n",
      "Episode 413 Reward: 23.700000000000074\n",
      "Episode 414 Reward: 16.99999999999999\n",
      "Episode 415 Reward: 18.50000000000001\n",
      "Episode 416 Reward: 22.70000000000006\n",
      "Episode 417 Reward: 15.299999999999967\n",
      "Episode 418 Reward: 22.10000000000005\n",
      "Episode 419 Reward: 37.20000000000022\n",
      "Episode 420 Reward: 34.10000000000018\n",
      "Episode 421 Reward: 23.100000000000065\n",
      "Episode 422 Reward: 23.700000000000088\n",
      "Episode 423 Reward: 22.300000000000054\n",
      "Episode 424 Reward: 31.500000000000142\n",
      "Episode 425 Reward: 17.39999999999999\n",
      "Timesteps: 87000, Policy Loss: -0.0022477273139015707, Value Loss: 10.450438404083252, Entropy: 0.9416211307048797, Combined Loss: 4.972197818756103\n",
      "Episode 426 Reward: 35.10000000000018\n",
      "Episode 427 Reward: 19.00000000000002\n",
      "Episode 428 Reward: 20.300000000000022\n",
      "Episode 429 Reward: 28.300000000000143\n",
      "Episode 430 Reward: 23.300000000000068\n",
      "Episode 431 Reward: 24.900000000000105\n",
      "Episode 432 Reward: 24.100000000000065\n",
      "Episode 433 Reward: 22.500000000000075\n",
      "Episode 434 Reward: 24.300000000000068\n",
      "Episode 435 Reward: 26.30000000000011\n",
      "Episode 436 Reward: 25.300000000000068\n",
      "Episode 437 Reward: 22.30000000000004\n",
      "Episode 438 Reward: 43.40000000000031\n",
      "Episode 439 Reward: 27.10000000000011\n",
      "Timesteps: 90000, Policy Loss: -0.0005159368657159825, Value Loss: 10.354005527496337, Entropy: 0.9632013738155365, Combined Loss: 5.096513080596924\n",
      "Episode 440 Reward: 22.300000000000054\n",
      "Episode 441 Reward: 24.900000000000077\n",
      "Episode 442 Reward: 24.700000000000088\n",
      "Episode 443 Reward: 26.500000000000103\n",
      "Episode 444 Reward: 26.700000000000106\n",
      "Episode 445 Reward: 22.70000000000006\n",
      "Episode 446 Reward: 29.500000000000142\n",
      "Episode 447 Reward: 21.300000000000036\n",
      "Episode 448 Reward: 21.10000000000005\n",
      "Episode 449 Reward: 29.500000000000128\n",
      "Episode 450 Reward: 23.300000000000054\n",
      "Episode 451 Reward: 22.400000000000055\n",
      "Episode 452 Reward: 19.000000000000004\n",
      "Timesteps: 93000, Policy Loss: -0.0022755402591389863, Value Loss: 9.64835262298584, Entropy: 0.9444398283958435, Combined Loss: 4.568289041519165\n",
      "Episode 453 Reward: 32.30000000000017\n",
      "Episode 454 Reward: 25.900000000000077\n",
      "Episode 455 Reward: 22.100000000000037\n",
      "Episode 456 Reward: 29.70000000000015\n",
      "Episode 457 Reward: 16.799999999999986\n",
      "Episode 458 Reward: 17.4\n",
      "Episode 459 Reward: 38.50000000000023\n",
      "Episode 460 Reward: 22.700000000000063\n",
      "Episode 461 Reward: 31.300000000000182\n",
      "Episode 462 Reward: 25.90000000000011\n",
      "Episode 463 Reward: 22.900000000000063\n",
      "Episode 464 Reward: 27.700000000000102\n",
      "Episode 465 Reward: 32.8000000000002\n",
      "Episode 466 Reward: 21.100000000000023\n",
      "Timesteps: 96000, Policy Loss: -7.871017033953543e-05, Value Loss: 10.93868465423584, Entropy: 0.9052950739860535, Combined Loss: 5.434312438964843\n",
      "Episode 467 Reward: 22.900000000000063\n",
      "Episode 468 Reward: 26.900000000000123\n",
      "Episode 469 Reward: 25.90000000000009\n",
      "Episode 470 Reward: 17.399999999999984\n",
      "Episode 471 Reward: 17.299999999999994\n",
      "Episode 472 Reward: 21.700000000000042\n",
      "Episode 473 Reward: 18.400000000000016\n",
      "Episode 474 Reward: 17.099999999999994\n",
      "Episode 475 Reward: 13.199999999999969\n",
      "Episode 476 Reward: 18.300000000000008\n",
      "Episode 477 Reward: 27.200000000000124\n",
      "Episode 478 Reward: 26.5000000000001\n",
      "Episode 479 Reward: 20.30000000000004\n",
      "Episode 480 Reward: 22.10000000000005\n",
      "Episode 481 Reward: 18.299999999999994\n",
      "Timesteps: 99000, Policy Loss: -0.0018538169877759358, Value Loss: 8.656493186950684, Entropy: 0.8561524093151093, Combined Loss: 4.117180252075196\n",
      "Episode 482 Reward: 30.100000000000172\n",
      "Episode 483 Reward: 12.09999999999997\n",
      "Episode 484 Reward: 33.50000000000016\n",
      "Episode 485 Reward: 22.300000000000054\n",
      "Episode 486 Reward: 17.4\n",
      "Episode 487 Reward: 25.10000000000008\n",
      "Episode 488 Reward: 19.50000000000002\n",
      "Episode 489 Reward: 21.10000000000005\n",
      "Episode 490 Reward: 25.50000000000007\n",
      "Episode 491 Reward: 27.90000000000009\n",
      "Episode 492 Reward: 25.500000000000103\n",
      "Episode 493 Reward: 21.300000000000036\n",
      "Episode 494 Reward: 13.399999999999968\n",
      "Episode 495 Reward: 26.70000000000009\n",
      "Episode 496 Reward: 22.10000000000005\n",
      "Timesteps: 102000, Policy Loss: -0.00015726604189889582, Value Loss: 8.585848999023437, Entropy: 0.7679604232311249, Combined Loss: 4.2541591167449955\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "env = VizDoomGym2(render=False)\n",
    "agent = PPOAgent(env.action_space)\n",
    "run_name= \"final_1\"\n",
    "ppo = PPO(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    run_name=run_name)\n",
    "total_timesteps = 100000\n",
    "ppo.learn(total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893666f3-9fae-4c6d-9af7-1c8b81ea4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'ppo_agent_{run_name}.pth'\n",
    "torch.save(agent.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9de620b2-e040-4dd2-aac3-2dbd0f1e4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configs\n",
    "config1 = 'github/VizDoom/scenarios/defend_the_center1.cfg'\n",
    "config2 = 'github/VizDoom/scenarios/defend_the_center2.cfg'\n",
    "config3 = 'github/VizDoom/scenarios/defend_the_center.cfg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3ed67632-b0cc-4a7c-a3d3-b009d0e589d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'ppo_vizdoom_agent_{run_name}.pth'\n",
    "torch.save(agent.state_dict(), filename)\n",
    "basic_file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2f56a120-ca49-4749-a08f-f31e7b9286ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 2 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 3 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 4 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 5 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy(agent, env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ca637-cf0a-48fc-8c95-f905069d66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cc575-ca90-4ca7-a077-05c2d9bc32e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
