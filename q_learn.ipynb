{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import vizdoom as vzd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ViZDoom environment\n",
    "def setup_vizdoom():\n",
    "    game = vzd.DoomGame()\n",
    "    game.set_doom_scenario_path(os.path.join(vzd.scenarios_path, \"basic.wad\"))\n",
    "    game.set_doom_map(\"map01\")\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_160X120)\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_available_buttons([vzd.Button.MOVE_LEFT, vzd.Button.MOVE_RIGHT, vzd.Button.ATTACK])\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_living_reward(-1)\n",
    "    game.init()\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, image_height: int, image_width: int, num_actions: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        h = image_height\n",
    "        w = image_width\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4)\n",
    "        h //= 4\n",
    "        w //= 4\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "        h //= 4\n",
    "        w //= 4\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(h * w * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, lr=0.0003, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        screen_shape = self.env.get_state().screen_buffer.shape\n",
    "        self.image_height, self.image_width, _ = screen_shape\n",
    "        self.num_actions = len(env.get_available_buttons())\n",
    "\n",
    "        self.q_network = QNetwork(self.image_height, self.image_width, self.num_actions)\n",
    "        self.target_network = QNetwork(self.image_height, self.image_width, self.num_actions)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.actions = [\n",
    "            [True, False, False],  # MOVE_LEFT\n",
    "            [False, True, False],  # MOVE_RIGHT\n",
    "            [False, False, True],  # ATTACK\n",
    "        ]\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)\n",
    "        obs = torch.tensor(obs, dtype=torch.float).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(obs)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def train(self, num_episodes, batch_size=32, target_update=10, replay_buffer_size=5000):\n",
    "        replay_buffer = []\n",
    "        for episode in range(num_episodes):\n",
    "            self.env.new_episode()\n",
    "            obs = self._process_obs(self.env.get_state().screen_buffer)\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not self.env.is_episode_finished():\n",
    "                action_idx = self.select_action(obs)\n",
    "                action = self.actions[action_idx]\n",
    "                reward = self.env.make_action(action)\n",
    "\n",
    "                next_obs = (\n",
    "                    self._process_obs(self.env.get_state().screen_buffer)\n",
    "                    if not self.env.is_episode_finished()\n",
    "                    else None\n",
    "                )\n",
    "                replay_buffer.append((obs, action_idx, reward, next_obs))\n",
    "                if len(replay_buffer) > replay_buffer_size:\n",
    "                    replay_buffer.pop(0)\n",
    "\n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    self._train_step(replay_buffer, batch_size)\n",
    "\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "            if episode % target_update == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "    def _train_step(self, replay_buffer, batch_size):\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch = zip(*batch)\n",
    "\n",
    "        obs_batch = torch.tensor(np.array(obs_batch), dtype=torch.float)\n",
    "        action_batch = torch.tensor(action_batch, dtype=torch.long)\n",
    "        reward_batch = torch.tensor(reward_batch, dtype=torch.float)\n",
    "        next_obs_batch = torch.tensor(\n",
    "            [x for x in next_obs_batch if x is not None], dtype=torch.float\n",
    "        )\n",
    "\n",
    "        q_values = self.q_network(obs_batch)\n",
    "        next_q_values = torch.zeros(len(batch), self.num_actions)\n",
    "        if len(next_obs_batch) > 0:\n",
    "            next_q_values[: len(next_obs_batch)] = self.target_network(next_obs_batch)\n",
    "\n",
    "        max_next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = reward_batch + (self.gamma * max_next_q_values)\n",
    "\n",
    "        current_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _process_obs(self, obs):\n",
    "        obs = obs.transpose(2, 0, 1)\n",
    "        obs = obs / 255.0  \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    env = setup_vizdoom()\n",
    "    agent = QLearningAgent(env)\n",
    "    agent.train(num_episodes=500)\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
