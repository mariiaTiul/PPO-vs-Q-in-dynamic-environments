{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import vizdoom as vzd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ViZDoom environment\n",
    "def setup_vizdoom():\n",
    "    game = vzd.DoomGame()\n",
    "    game.set_doom_scenario_path(os.path.join(vzd.scenarios_path, \"basic.wad\"))\n",
    "    game.set_doom_map(\"map01\")\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_160X120)\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_available_buttons([vzd.Button.MOVE_LEFT, vzd.Button.MOVE_RIGHT, vzd.Button.ATTACK])\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_living_reward(-1)\n",
    "    # game.set_reward_for_kill(10)\n",
    "    game.init()\n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QNetwork(nn.Module):\n",
    "#     def __init__(self, image_height: int, image_width: int, num_actions: int):\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         h = image_height\n",
    "#         w = image_width\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "#         self.pool1 = nn.MaxPool2d(kernel_size=4)\n",
    "#         h //= 4\n",
    "#         w //= 4\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "#         h //= 4\n",
    "#         w //= 4\n",
    "\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(h * w * 32, 128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, num_actions)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.pool2(x)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         return self.fc(x)\n",
    "\n",
    "# Neural network for approximating Q-values\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        h, w, c = input_shape\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * (h // 4 - 1) * (w // 4 - 1), 128)\n",
    "        self.fc2 = nn.Linear(128, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, lr=0.0001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1):\n",
    "        self.env = env\n",
    "        self.input_shape = env.get_state().screen_buffer.shape  # (H, W, C)\n",
    "        self.num_actions = len(env.get_available_buttons())\n",
    "\n",
    "        # Define action mappings explicitly\n",
    "        self.actions = [\n",
    "            [True, False, False],  # MOVE_LEFT\n",
    "            [False, True, False],  # MOVE_RIGHT\n",
    "            [False, False, True],  # ATTACK\n",
    "        ]\n",
    "\n",
    "        self.q_network = QNetwork(self.input_shape, self.num_actions)\n",
    "        self.target_network = QNetwork(self.input_shape, self.num_actions)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_size = 5000\n",
    "        self.batch_size = 32\n",
    "        self.target_update_freq = 10\n",
    "\n",
    "    def preprocess_observation(self, obs):\n",
    "        obs = obs.transpose(2, 0, 1)  # Channels-first for PyTorch\n",
    "        obs = obs / 255.0  # Normalize pixel values to [0, 1]\n",
    "        return torch.tensor(obs, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.num_actions - 1)  # Explore\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(obs)\n",
    "        return torch.argmax(q_values).item()  # Exploit\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*batch)\n",
    "\n",
    "        obs_batch = torch.cat(obs_batch)\n",
    "        next_obs_batch = torch.cat(next_obs_batch)\n",
    "        action_batch = torch.tensor(action_batch, dtype=torch.long)\n",
    "        reward_batch = torch.tensor(reward_batch, dtype=torch.float)\n",
    "        done_batch = torch.tensor(done_batch, dtype=torch.float)\n",
    "\n",
    "        q_values = self.q_network(obs_batch)\n",
    "        next_q_values = self.target_network(next_obs_batch)\n",
    "\n",
    "        max_next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "        target_q_values = reward_batch + self.gamma * max_next_q_values * (1 - done_batch)\n",
    "\n",
    "        current_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for episode in range(num_episodes):\n",
    "            self.env.new_episode()\n",
    "            obs = self.preprocess_observation(self.env.get_state().screen_buffer)\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not self.env.is_episode_finished():\n",
    "                action = self.select_action(obs)\n",
    "                reward = self.env.make_action(self.actions[action])  # FIXED LINE\n",
    "                episode_reward += reward\n",
    "\n",
    "                done = self.env.is_episode_finished()\n",
    "                next_obs = (\n",
    "                    self.preprocess_observation(self.env.get_state().screen_buffer)\n",
    "                    if not done\n",
    "                    else torch.zeros_like(obs)\n",
    "                )\n",
    "\n",
    "                self.replay_buffer.append((obs, action, reward, next_obs, done))\n",
    "                if len(self.replay_buffer) > self.buffer_size:\n",
    "                    self.replay_buffer.pop(0)\n",
    "\n",
    "                obs = next_obs\n",
    "                self.train_step()\n",
    "\n",
    "            if episode % self.target_update_freq == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            if episode % 50 == 0:\n",
    "                self.epsilon = 1.0  # Explore more often\n",
    "\n",
    "            print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {self.epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QLearningAgent:\n",
    "#     def __init__(self, env, lr=0.0003, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "#         self.env = env\n",
    "#         screen_shape = self.env.get_state().screen_buffer.shape\n",
    "#         self.image_height, self.image_width, _ = screen_shape\n",
    "#         self.num_actions = len(env.get_available_buttons())\n",
    "\n",
    "#         self.q_network = QNetwork(self.image_height, self.image_width, self.num_actions)\n",
    "#         self.target_network = QNetwork(self.image_height, self.image_width, self.num_actions)\n",
    "#         self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "#         self.target_network.eval()\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "#         self.gamma = gamma\n",
    "#         self.epsilon = epsilon\n",
    "#         self.epsilon_decay = epsilon_decay\n",
    "#         self.epsilon_min = epsilon_min\n",
    "#         self.actions = [\n",
    "#             [True, False, False],  # MOVE_LEFT\n",
    "#             [False, True, False],  # MOVE_RIGHT\n",
    "#             [False, False, True],  # ATTACK\n",
    "#         ]\n",
    "\n",
    "#     def select_action(self, obs):\n",
    "#         if random.random() < self.epsilon:\n",
    "#             return random.randint(0, self.num_actions - 1)\n",
    "#         obs = torch.tensor(obs, dtype=torch.float).unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             q_values = self.q_network(obs)\n",
    "#         return torch.argmax(q_values).item()\n",
    "\n",
    "#     def train(self, num_episodes, batch_size=32, target_update=10, replay_buffer_size=5000):\n",
    "#         replay_buffer = []\n",
    "#         for episode in range(num_episodes):\n",
    "#             self.env.new_episode()\n",
    "#             obs = self._process_obs(self.env.get_state().screen_buffer)\n",
    "#             episode_reward = 0\n",
    "#             previous_killcount = 0\n",
    "#             episode_rewards = []\n",
    "\n",
    "#             while not self.env.is_episode_finished():\n",
    "#                 action_idx = self.select_action(obs)\n",
    "#                 action = self.actions[action_idx]\n",
    "#                 reward = self.env.make_action(action)\n",
    "#                 print(f\"Reward received: {reward}\")\n",
    "\n",
    "#                 current_killcount = self.env.get_game_variable(vzd.GameVariable.KILLCOUNT)\n",
    "#                 if current_killcount > previous_killcount:\n",
    "#                     reward += 10\n",
    "#                     previous_killcount = current_killcount\n",
    "\n",
    "#                 next_obs = (\n",
    "#                     self._process_obs(self.env.get_state().screen_buffer)\n",
    "#                     if not self.env.is_episode_finished()\n",
    "#                     else None\n",
    "#                 )\n",
    "#                 replay_buffer.append((obs, action_idx, reward, next_obs))\n",
    "#                 if len(replay_buffer) > replay_buffer_size:\n",
    "#                     replay_buffer.pop(0)\n",
    "                \n",
    "#                 if len(episode_rewards) % 100 == 0:\n",
    "#                     print(f\"Intermediate reward (last 100 timesteps): {sum(episode_rewards[-100:])}\")\n",
    "\n",
    "#                 obs = next_obs\n",
    "#                 episode_reward += reward\n",
    "#                 episode_rewards.append(episode_reward)\n",
    "\n",
    "#                 if len(replay_buffer) >= batch_size:\n",
    "#                     self._train_step(replay_buffer, batch_size)\n",
    "\n",
    "#             self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "#             if episode % target_update == 0:\n",
    "#                 self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "#             print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "#     def _train_step(self, replay_buffer, batch_size):\n",
    "#         batch = random.sample(replay_buffer, batch_size)\n",
    "#         obs_batch, action_batch, reward_batch, next_obs_batch = zip(*batch)\n",
    "\n",
    "#         obs_batch = torch.tensor(np.array(obs_batch), dtype=torch.float)\n",
    "#         action_batch = torch.tensor(action_batch, dtype=torch.long)\n",
    "#         reward_batch = torch.tensor(reward_batch, dtype=torch.float)\n",
    "#         next_obs_batch = torch.tensor(\n",
    "#             [x for x in next_obs_batch if x is not None], dtype=torch.float\n",
    "#         )\n",
    "\n",
    "#         q_values = self.q_network(obs_batch)\n",
    "#         next_q_values = torch.zeros(len(batch), self.num_actions)\n",
    "#         if len(next_obs_batch) > 0:\n",
    "#             next_q_values[: len(next_obs_batch)] = self.target_network(next_obs_batch)\n",
    "\n",
    "#         max_next_q_values = torch.max(next_q_values, dim=1)[0]\n",
    "#         target_q_values = reward_batch + (self.gamma * max_next_q_values)\n",
    "\n",
    "#         current_q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "#         loss = F.mse_loss(current_q_values, target_q_values)\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#     def _process_obs(self, obs):\n",
    "#         obs = obs.transpose(2, 0, 1)\n",
    "#         obs = obs / 255.0  \n",
    "#         return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Reward: 54.0, Epsilon: 1.00\n",
      "Episode 2/500, Reward: -245.0, Epsilon: 0.99\n",
      "Episode 3/500, Reward: -150.0, Epsilon: 0.99\n",
      "Episode 4/500, Reward: 82.0, Epsilon: 0.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m setup_vizdoom()\n\u001b[1;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(env)\n\u001b[0;32m----> 5\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m      7\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[35], line 93\u001b[0m, in \u001b[0;36mQLearningAgent.train\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m     obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[35], line 58\u001b[0m, in \u001b[0;36mQLearningAgent.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m done_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done_batch, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m     57\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network(obs_batch)\n\u001b[0;32m---> 58\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_network(next_obs_batch)\n\u001b[1;32m     60\u001b[0m max_next_q_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_q_values, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     61\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m max_next_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done_batch)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 44\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    env = setup_vizdoom()\n",
    "    agent = QLearningAgent(env)\n",
    "    agent.train(num_episodes=500)\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
