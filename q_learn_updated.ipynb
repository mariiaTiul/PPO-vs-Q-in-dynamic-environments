{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: github: No such file or directory\n",
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Applications/anaconda3/lib/python3.11/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Applications/anaconda3/lib/python3.11/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Applications/anaconda3/lib/python3.11/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: opencv-python in /Applications/anaconda3/lib/python3.11/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Applications/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting stable-baselines3[extra]\n",
      "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.0.1)\n",
      "Requirement already satisfied: cloudpickle in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.2.1)\n",
      "Requirement already satisfied: pandas in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (3.8.0)\n",
      "Requirement already satisfied: opencv-python in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.9.0.80)\n",
      "Requirement already satisfied: pygame in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (4.65.0)\n",
      "Requirement already satisfied: rich in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (13.3.5)\n",
      "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
      "  Downloading ale_py-0.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pillow in /Applications/anaconda3/lib/python3.11/site-packages (from stable-baselines3[extra]) (10.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Applications/anaconda3/lib/python3.11/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (4.9.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.0)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: packaging in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.6.0)\n",
      "Requirement already satisfied: six>1.9 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.11/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Applications/anaconda3/lib/python3.11/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Applications/anaconda3/lib/python3.11/site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.11/site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Applications/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Downloading ale_py-0.10.1-cp311-cp311-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m173.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m174.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:14\u001b[0m\n",
      "\u001b[?25hDownloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
      "Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m163.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.0-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: tensorboard-data-server, protobuf, grpcio, ale-py, tensorboard, stable-baselines3\n",
      "Successfully installed ale-py-0.10.1 grpcio-1.68.0 protobuf-5.29.0 stable-baselines3-2.4.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2\n",
      "Requirement already satisfied: torch in /Applications/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Applications/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Applications/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/basic.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.expand_dims(resized, axis=0) \n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(qlearning_model, env, trials=5):\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            # Get action\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = qlearning_model.q_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Take step in env\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {trials} evaluation episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, run_name, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, buffer_size=10000, batch_size=32):\n",
    "        self.env = env\n",
    "        self.q_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        log_dir = os.path.join(\"qlearning_logs\", run_name)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.env.action_space.n - 1)\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        if len(self.replay_buffer) >= self.buffer_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "        state = np.squeeze(state, axis=0)  # too many dimensions fix\n",
    "        next_state = np.squeeze(next_state, axis=0)  # too many dimensions fix\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)).to(device),\n",
    "            torch.LongTensor(actions).unsqueeze(1).to(device),\n",
    "            torch.FloatTensor(rewards).to(device),\n",
    "            torch.FloatTensor(np.array(next_states)).to(device),\n",
    "            torch.FloatTensor(dones).to(device),\n",
    "        )\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "\n",
    "        current_q_values = self.q_network(states).gather(1, actions)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "\n",
    "        # Log loss to TensorBoard\n",
    "        self.writer.add_scalar(\"Loss\", loss.item(), self.global_step)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        timesteps = 0\n",
    "        episode_reward = 0\n",
    "        episode_rewards = []\n",
    "        episode = 0\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "            self.store_transition(state, action, reward, next_state, terminated)\n",
    "            self.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            timesteps += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            if terminated:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "                if len(episode_rewards) >= 10:\n",
    "                    avg_reward = np.mean(episode_rewards[-10:])\n",
    "                else:\n",
    "                    avg_reward = np.mean(episode_rewards)\n",
    "                self.writer.add_scalar(\"Average Reward\", avg_reward, self.global_step)\n",
    "\n",
    "                episode_reward = 0\n",
    "                episode += 1\n",
    "                state, _ = self.env.reset()\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "\n",
    "            if timesteps % 1000 == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningCallback:\n",
    "    def __init__(self, check_freq, save_path, eval_env, eval_trials=5, verbose=1):\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_trials = eval_trials\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self, qlearning_model, timesteps):\n",
    "        if timesteps % self.check_freq == 0:\n",
    "            # Save model checkpoint\n",
    "            model_path = os.path.join(self.save_path, f'best_model_{timesteps}.pth')\n",
    "            torch.save(qlearning_model.q_network.state_dict(), model_path)\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved at step {timesteps}\")\n",
    "\n",
    "            # Evaluate policy\n",
    "            avg_reward = evaluate_policy(qlearning_model, self.eval_env, trials=self.eval_trials)\n",
    "            if self.verbose:\n",
    "                print(f\"Average Reward after {timesteps} timesteps: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 62.0\n",
      "Episode 1 Reward: 64.0\n",
      "Episode 2 Reward: 95.0\n",
      "Episode 3 Reward: -380.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=False)\n",
    "    run_name = \"qlearning_basic\"\n",
    "    qlearning = QLearning(env, run_name)\n",
    "\n",
    "    callback = QLearningCallback(check_freq=1000, save_path=\"qlearning_checkpoints\", eval_env = env, verbose=1)\n",
    "\n",
    "    qlearning.train(total_timesteps=50000)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = f'final_qlearning_model_{run_name}.pth'\n",
    "    torch.save(qlearning.q_network.state_dict(), final_model_path)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = VizDoomGym(render=False)\n",
    "callback = QLearningCallback(\n",
    "        check_freq=5000, \n",
    "        save_path=\"qlearning_checkpoints\", \n",
    "        eval_env=eval_env, \n",
    "        eval_trials=5, \n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 67.0\n",
      "Episode 2 Reward: 91.0\n",
      "Episode 3 Reward: 67.0\n",
      "Episode 4 Reward: 67.0\n",
      "Episode 5 Reward: 75.0\n",
      "Average Reward over 5 evaluation episodes: 73.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(qlearning, eval_env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
