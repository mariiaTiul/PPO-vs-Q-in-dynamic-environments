{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\19096\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.9.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
      "Requirement already satisfied: pygame in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.18.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.66.5)\n",
      "Requirement already satisfied: rich in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.7.1)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.10.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: torch in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym1(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.expand_dims(resized, axis=0) \n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym2(Env):\n",
    "    def __init__(self, render=False, config='github/VizDoom/scenarios/defend_the_center.cfg'):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(config)\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "        #self.ammo = 26\n",
    "        self.living_reward = 0.1\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        base_reward = self.game.make_action(actions[action], 2)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "            ammo = self.game.get_state().game_variables[0]\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "\n",
    "        reward = base_reward + self.living_reward\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized  = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.reshape(resized, (1, 100, 160))\n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(qlearning_model, env, trials=5):\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            # Get action\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = qlearning_model.q_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Take step in env\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {trials} evaluation episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, run_name, lr=1e-4, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, buffer_size=50000, batch_size=64):\n",
    "        self.env = env\n",
    "        self.q_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        log_dir = os.path.join(\"qlearning_logs\", run_name)\n",
    "        self.losses = deque(maxlen=100)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Ensure state has shape [channels, height, width]\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # Ensure states have correct shape\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        if len(next_state.shape) == 2:\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "        current_q_values = self.q_network(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.losses.append(loss.item())\n",
    "        avg_loss = np.mean(self.losses)\n",
    "        self.writer.add_scalar(\"Average Loss\", avg_loss, self.global_step)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state / 255.0  # Normalize initial state\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "\n",
    "        timesteps = 0\n",
    "        episode_reward = 0\n",
    "        episode_rewards = []\n",
    "        episode = 0\n",
    "\n",
    "        # Pre-fill replay buffer\n",
    "        print(\"Pre-filling replay buffer...\")\n",
    "        while len(self.replay_buffer) < self.batch_size:\n",
    "            action = random.randint(0, self.env.action_space.n - 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state / 255.0\n",
    "            if len(next_state.shape) == 2:\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "            self.store_transition(state, action, reward, next_state, terminated)\n",
    "\n",
    "            if terminated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = state / 255.0\n",
    "                if len(state.shape) == 2:\n",
    "                    state = np.expand_dims(state, axis=0)\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        print(\"Replay buffer pre-filled. Starting training...\")\n",
    "\n",
    "        # Main training loop\n",
    "        while timesteps < total_timesteps:\n",
    "            action = self.select_action(state) if random.random() > self.epsilon else random.randint(0, self.env.action_space.n - 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state / 255.0\n",
    "            if len(next_state.shape) == 2:\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "            self.store_transition(state, action, reward, next_state, terminated)\n",
    "            self.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            timesteps += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            if terminated:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "                if len(episode_rewards) >= 100:\n",
    "                    avg_reward = np.mean(episode_rewards[-100:])\n",
    "                else:\n",
    "                    avg_reward = np.mean(episode_rewards)\n",
    "                self.writer.add_scalar(\"Average Reward\", avg_reward, self.global_step)\n",
    "                self.writer.add_scalar(\"Epsilon\", self.epsilon, self.global_step)\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "                episode_reward = 0\n",
    "                episode += 1\n",
    "                state, _ = self.env.reset()\n",
    "                state = state / 255.0\n",
    "                if len(state.shape) == 2:\n",
    "                    state = np.expand_dims(state, axis=0)\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "            if timesteps % 500 == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningCallback:\n",
    "    def __init__(self, check_freq, save_path, eval_env, eval_trials=5, verbose=1):\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_trials = eval_trials\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self, qlearning_model, timesteps):\n",
    "        if timesteps % self.check_freq == 0:\n",
    "            # Save model checkpoint\n",
    "            model_path = os.path.join(self.save_path, f'best_model_{timesteps}.pth')\n",
    "            torch.save(qlearning_model.q_network.state_dict(), model_path)\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved at step {timesteps}\")\n",
    "\n",
    "            # Evaluate policy\n",
    "            avg_reward = evaluate_policy(qlearning_model, self.eval_env, trials=self.eval_trials)\n",
    "            if self.verbose:\n",
    "                print(f\"Average Reward after {timesteps} timesteps: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-filling replay buffer...\n",
      "Replay buffer pre-filled. Starting training...\n",
      "Episode 0 Reward: 10.799999999999978\n",
      "Episode 1 Reward: 10.09999999999998\n",
      "Episode 2 Reward: 17.49999999999998\n",
      "Episode 3 Reward: 20.90000000000005\n",
      "Episode 4 Reward: 14.199999999999962\n",
      "Episode 5 Reward: 21.10000000000005\n",
      "Episode 6 Reward: 19.900000000000023\n",
      "Episode 7 Reward: 16.199999999999978\n",
      "Episode 8 Reward: 13.299999999999969\n",
      "Episode 9 Reward: 12.09999999999997\n",
      "Episode 10 Reward: 12.09999999999997\n",
      "Episode 11 Reward: 15.29999999999996\n",
      "Episode 12 Reward: 16.199999999999978\n",
      "Episode 13 Reward: 22.500000000000053\n",
      "Episode 14 Reward: 16.899999999999988\n",
      "Episode 15 Reward: 17.499999999999996\n",
      "Episode 16 Reward: 14.89999999999996\n",
      "Episode 17 Reward: 14.399999999999961\n",
      "Episode 18 Reward: 18.499999999999996\n",
      "Episode 19 Reward: 11.99999999999997\n",
      "Episode 20 Reward: 18.50000000000001\n",
      "Episode 21 Reward: 12.09999999999997\n",
      "Episode 22 Reward: 11.199999999999973\n",
      "Episode 23 Reward: 16.199999999999978\n",
      "Episode 24 Reward: 17.19999999999998\n",
      "Episode 25 Reward: 16.799999999999986\n",
      "Episode 26 Reward: 12.09999999999997\n",
      "Episode 27 Reward: 20.100000000000033\n",
      "Episode 28 Reward: 23.90000000000009\n",
      "Episode 29 Reward: 16.199999999999978\n",
      "Episode 30 Reward: 17.199999999999996\n",
      "Episode 31 Reward: 15.799999999999967\n",
      "Episode 32 Reward: 17.099999999999994\n",
      "Episode 33 Reward: 16.199999999999978\n",
      "Episode 34 Reward: 11.99999999999997\n",
      "Episode 35 Reward: 16.199999999999978\n",
      "Episode 36 Reward: 18.200000000000006\n",
      "Episode 37 Reward: 13.09999999999997\n",
      "Episode 38 Reward: 16.199999999999978\n",
      "Episode 39 Reward: 17.299999999999997\n",
      "Episode 40 Reward: 17.199999999999978\n",
      "Episode 41 Reward: 18.700000000000014\n",
      "Episode 42 Reward: 25.300000000000107\n",
      "Episode 43 Reward: 12.09999999999997\n",
      "Episode 44 Reward: 17.7\n",
      "Episode 45 Reward: 12.09999999999997\n",
      "Episode 46 Reward: 21.500000000000057\n",
      "Episode 47 Reward: 14.199999999999966\n",
      "Episode 48 Reward: 19.30000000000001\n",
      "Episode 49 Reward: 21.700000000000042\n",
      "Episode 50 Reward: 12.09999999999997\n",
      "Episode 51 Reward: 12.09999999999997\n",
      "Episode 52 Reward: 17.199999999999996\n",
      "Episode 53 Reward: 20.400000000000023\n",
      "Episode 54 Reward: 21.100000000000033\n",
      "Episode 55 Reward: 12.899999999999968\n",
      "Episode 56 Reward: 16.199999999999978\n",
      "Episode 57 Reward: 12.899999999999967\n",
      "Episode 58 Reward: 16.199999999999978\n",
      "Episode 59 Reward: 11.499999999999972\n",
      "Episode 60 Reward: 23.900000000000095\n",
      "Episode 61 Reward: 13.09999999999997\n",
      "Episode 62 Reward: 21.900000000000063\n",
      "Episode 63 Reward: 12.89999999999997\n",
      "Episode 64 Reward: 12.09999999999997\n",
      "Episode 65 Reward: 22.100000000000065\n",
      "Episode 66 Reward: 15.899999999999972\n",
      "Episode 67 Reward: 16.199999999999978\n",
      "Episode 68 Reward: 16.099999999999977\n",
      "Episode 69 Reward: 13.999999999999966\n",
      "Episode 70 Reward: 16.199999999999978\n",
      "Episode 71 Reward: 16.69999999999998\n",
      "Episode 72 Reward: 24.900000000000073\n",
      "Episode 73 Reward: 12.09999999999997\n",
      "Episode 74 Reward: 18.600000000000012\n",
      "Episode 75 Reward: 12.399999999999968\n",
      "Episode 76 Reward: 16.29999999999996\n",
      "Episode 77 Reward: 12.09999999999997\n",
      "Episode 78 Reward: 16.199999999999978\n",
      "Episode 79 Reward: 17.199999999999996\n",
      "Episode 80 Reward: 21.000000000000046\n",
      "Episode 81 Reward: 16.199999999999978\n",
      "Episode 82 Reward: 21.70000000000003\n",
      "Episode 83 Reward: 14.199999999999966\n",
      "Episode 84 Reward: 16.199999999999978\n",
      "Episode 85 Reward: 12.399999999999968\n",
      "Episode 86 Reward: 12.09999999999997\n",
      "Episode 87 Reward: 13.299999999999965\n",
      "Episode 88 Reward: 12.199999999999969\n",
      "Episode 89 Reward: 12.999999999999966\n",
      "Episode 90 Reward: 10.599999999999978\n",
      "Episode 91 Reward: 14.89999999999996\n",
      "Episode 92 Reward: 12.09999999999997\n",
      "Episode 93 Reward: 16.799999999999972\n",
      "Episode 94 Reward: 16.39999999999998\n",
      "Episode 95 Reward: 17.19999999999998\n",
      "Episode 96 Reward: 14.09999999999997\n",
      "Episode 97 Reward: 18.400000000000006\n",
      "Episode 98 Reward: 16.199999999999978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m qlearning \u001b[38;5;241m=\u001b[39m QLearning(env, run_name)\n\u001b[0;32m      5\u001b[0m callback \u001b[38;5;241m=\u001b[39m QLearningCallback(check_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqlearning_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_env \u001b[38;5;241m=\u001b[39m env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m qlearning\u001b[38;5;241m.\u001b[39mtrain(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 141\u001b[0m, in \u001b[0;36mQLearning.train\u001b[1;34m(self, total_timesteps)\u001b[0m\n\u001b[0;32m    138\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(next_state, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state, terminated)\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m    143\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    144\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[1;32mIn[15], line 99\u001b[0m, in \u001b[0;36mQLearning.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     98\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_network\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    env = VizDoomGym2(render=False)\n",
    "    run_name = \"qlearning_defend_13\"\n",
    "    qlearning = QLearning(env, run_name)\n",
    "\n",
    "    callback = QLearningCallback(check_freq=1000, save_path=\"qlearning_checkpoints\", eval_env = env, verbose=1)\n",
    "\n",
    "    qlearning.train(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 62.0\n",
      "Episode 1 Reward: 64.0\n",
      "Episode 2 Reward: 95.0\n",
      "Episode 3 Reward: -380.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=False)\n",
    "    run_name = \"qlearning_defend_2\"\n",
    "    qlearning = QLearning(env, run_name)\n",
    "\n",
    "    callback = QLearningCallback(check_freq=1000, save_path=\"qlearning_checkpoints\", eval_env = env, verbose=1)\n",
    "\n",
    "    qlearning.train(total_timesteps=100000)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = f'final_qlearning_model_{run_name}.pth'\n",
    "    torch.save(qlearning.q_network.state_dict(), final_model_path)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = VizDoomGym(render=False)\n",
    "callback = QLearningCallback(\n",
    "        check_freq=5000, \n",
    "        save_path=\"qlearning_checkpoints\", \n",
    "        eval_env=eval_env, \n",
    "        eval_trials=5, \n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 67.0\n",
      "Episode 2 Reward: 91.0\n",
      "Episode 3 Reward: 67.0\n",
      "Episode 4 Reward: 67.0\n",
      "Episode 5 Reward: 75.0\n",
      "Average Reward over 5 evaluation episodes: 73.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(qlearning, eval_env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
