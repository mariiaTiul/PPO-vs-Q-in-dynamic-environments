{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\19096\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.9.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
      "Requirement already satisfied: pygame in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.18.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.66.5)\n",
      "Requirement already satisfied: rich in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.7.1)\n",
      "Requirement already satisfied: ale-py>=0.9.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.10.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: torch in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.expand_dims(resized, axis=0) \n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(qlearning_model, env, trials=5):\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        total_reward = 0\n",
    "        terminated = False\n",
    "\n",
    "        while not terminated:\n",
    "            # Get action\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_values = qlearning_model.q_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Take step in env\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward over {trials} evaluation episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, run_name, lr=1e-5, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, buffer_size=50000, batch_size=32):\n",
    "        self.env = env\n",
    "        self.q_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network = QNetwork(env.action_space).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        log_dir = os.path.join(\"qlearning_logs\", run_name)\n",
    "        self.losses = deque(maxlen=100)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Ensure state has shape [channels, height, width]\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        # Ensure states have correct shape\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "        if len(next_state.shape) == 2:\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.FloatTensor(np.array(states)).to(device)\n",
    "        next_states = torch.FloatTensor(np.array(next_states)).to(device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.sample_batch()\n",
    "        current_q_values = self.q_network(states).gather(1, actions)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        loss = nn.MSELoss()(current_q_values, target_q_values)\n",
    "\n",
    "        self.losses.append(loss.item())\n",
    "        avg_loss = np.mean(self.losses)\n",
    "        self.writer.add_scalar(\"Average Loss\", avg_loss, self.global_step)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = state / 255.0  # Normalize initial state\n",
    "        if len(state.shape) == 2:\n",
    "            state = np.expand_dims(state, axis=0)  # Add channel dimension\n",
    "\n",
    "        timesteps = 0\n",
    "        episode_reward = 0\n",
    "        episode_rewards = []\n",
    "        episode = 0\n",
    "\n",
    "        # Pre-fill replay buffer\n",
    "        print(\"Pre-filling replay buffer...\")\n",
    "        while len(self.replay_buffer) < self.batch_size:\n",
    "            action = random.randint(0, self.env.action_space.n - 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state / 255.0\n",
    "            if len(next_state.shape) == 2:\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "            self.store_transition(state, action, reward, next_state, terminated)\n",
    "\n",
    "            if terminated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = state / 255.0\n",
    "                if len(state.shape) == 2:\n",
    "                    state = np.expand_dims(state, axis=0)\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        print(\"Replay buffer pre-filled. Starting training...\")\n",
    "\n",
    "        # Main training loop\n",
    "        while timesteps < total_timesteps:\n",
    "            action = self.select_action(state) if random.random() > self.epsilon else random.randint(0, self.env.action_space.n - 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            next_state = next_state / 255.0\n",
    "            if len(next_state.shape) == 2:\n",
    "                next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "            self.store_transition(state, action, reward, next_state, terminated)\n",
    "            self.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            timesteps += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            if terminated:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "                if len(episode_rewards) >= 10:\n",
    "                    avg_reward = np.mean(episode_rewards[-10:])\n",
    "                else:\n",
    "                    avg_reward = np.mean(episode_rewards)\n",
    "                self.writer.add_scalar(\"Average Reward\", avg_reward, self.global_step)\n",
    "                self.writer.add_scalar(\"Epsilon\", self.epsilon, self.global_step)\n",
    "                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "                episode_reward = 0\n",
    "                episode += 1\n",
    "                state, _ = self.env.reset()\n",
    "                state = state / 255.0\n",
    "                if len(state.shape) == 2:\n",
    "                    state = np.expand_dims(state, axis=0)\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "            if timesteps % 500 == 0:\n",
    "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningCallback:\n",
    "    def __init__(self, check_freq, save_path, eval_env, eval_trials=5, verbose=1):\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_trials = eval_trials\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self, qlearning_model, timesteps):\n",
    "        if timesteps % self.check_freq == 0:\n",
    "            # Save model checkpoint\n",
    "            model_path = os.path.join(self.save_path, f'best_model_{timesteps}.pth')\n",
    "            torch.save(qlearning_model.q_network.state_dict(), model_path)\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved at step {timesteps}\")\n",
    "\n",
    "            # Evaluate policy\n",
    "            avg_reward = evaluate_policy(qlearning_model, self.eval_env, trials=self.eval_trials)\n",
    "            if self.verbose:\n",
    "                print(f\"Average Reward after {timesteps} timesteps: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-filling replay buffer...\n",
      "Replay buffer pre-filled. Starting training...\n",
      "Episode 0 Reward: 0.0\n",
      "Episode 1 Reward: 0.0\n",
      "Episode 2 Reward: 0.0\n",
      "Episode 3 Reward: 0.0\n",
      "Episode 4 Reward: 0.0\n",
      "Episode 5 Reward: -1.0\n",
      "Episode 6 Reward: -1.0\n",
      "Episode 7 Reward: 2.0\n",
      "Episode 8 Reward: 0.0\n",
      "Episode 9 Reward: -1.0\n",
      "Episode 10 Reward: 0.0\n",
      "Episode 11 Reward: 0.0\n",
      "Episode 12 Reward: 0.0\n",
      "Episode 13 Reward: 0.0\n",
      "Episode 14 Reward: 2.0\n",
      "Episode 15 Reward: -1.0\n",
      "Episode 16 Reward: 0.0\n",
      "Episode 17 Reward: 0.0\n",
      "Episode 18 Reward: 0.0\n",
      "Episode 19 Reward: -1.0\n",
      "Episode 20 Reward: 2.0\n",
      "Episode 21 Reward: 0.0\n",
      "Episode 22 Reward: -1.0\n",
      "Episode 23 Reward: 0.0\n",
      "Episode 24 Reward: 0.0\n",
      "Episode 25 Reward: 1.0\n",
      "Episode 26 Reward: 2.0\n",
      "Episode 27 Reward: 2.0\n",
      "Episode 28 Reward: 0.0\n",
      "Episode 29 Reward: -1.0\n",
      "Episode 30 Reward: 0.0\n",
      "Episode 31 Reward: 0.0\n",
      "Episode 32 Reward: -1.0\n",
      "Episode 33 Reward: 1.0\n",
      "Episode 34 Reward: 1.0\n",
      "Episode 35 Reward: 0.0\n",
      "Episode 36 Reward: 0.0\n",
      "Episode 37 Reward: 0.0\n",
      "Episode 38 Reward: 0.0\n",
      "Episode 39 Reward: 1.0\n",
      "Episode 40 Reward: 0.0\n",
      "Episode 41 Reward: 0.0\n",
      "Episode 42 Reward: 1.0\n",
      "Episode 43 Reward: 1.0\n",
      "Episode 44 Reward: 1.0\n",
      "Episode 45 Reward: 0.0\n",
      "Episode 46 Reward: 1.0\n",
      "Episode 47 Reward: 1.0\n",
      "Episode 48 Reward: -1.0\n",
      "Episode 49 Reward: 1.0\n",
      "Episode 50 Reward: 0.0\n",
      "Episode 51 Reward: 0.0\n",
      "Episode 52 Reward: 2.0\n",
      "Episode 53 Reward: 1.0\n",
      "Episode 54 Reward: 0.0\n",
      "Episode 55 Reward: 0.0\n",
      "Episode 56 Reward: 0.0\n",
      "Episode 57 Reward: 3.0\n",
      "Episode 58 Reward: 0.0\n",
      "Episode 59 Reward: -1.0\n",
      "Episode 60 Reward: 1.0\n",
      "Episode 61 Reward: 0.0\n",
      "Episode 62 Reward: 2.0\n",
      "Episode 63 Reward: 1.0\n",
      "Episode 64 Reward: 1.0\n",
      "Episode 65 Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "    env = VizDoomGym(render=False)\n",
    "    run_name = \"qlearning_defend_8\"\n",
    "    qlearning = QLearning(env, run_name)\n",
    "\n",
    "    callback = QLearningCallback(check_freq=1000, save_path=\"qlearning_checkpoints\", eval_env = env, verbose=1)\n",
    "\n",
    "    qlearning.train(total_timesteps=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 62.0\n",
      "Episode 1 Reward: 64.0\n",
      "Episode 2 Reward: 95.0\n",
      "Episode 3 Reward: -380.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = VizDoomGym(render=False)\n",
    "    run_name = \"qlearning_defend_2\"\n",
    "    qlearning = QLearning(env, run_name)\n",
    "\n",
    "    callback = QLearningCallback(check_freq=1000, save_path=\"qlearning_checkpoints\", eval_env = env, verbose=1)\n",
    "\n",
    "    qlearning.train(total_timesteps=100000)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = f'final_qlearning_model_{run_name}.pth'\n",
    "    torch.save(qlearning.q_network.state_dict(), final_model_path)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = VizDoomGym(render=False)\n",
    "callback = QLearningCallback(\n",
    "        check_freq=5000, \n",
    "        save_path=\"qlearning_checkpoints\", \n",
    "        eval_env=eval_env, \n",
    "        eval_trials=5, \n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: 67.0\n",
      "Episode 2 Reward: 91.0\n",
      "Episode 3 Reward: 67.0\n",
      "Episode 4 Reward: 67.0\n",
      "Episode 5 Reward: 75.0\n",
      "Average Reward over 5 evaluation episodes: 73.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(qlearning, eval_env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
