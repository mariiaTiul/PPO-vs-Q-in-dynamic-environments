{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e995f635-12db-4c27-a64e-29e3014c1e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from distutils.util import strtobool\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import MultivariateNormal\n",
    "import vizdoom as vzd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb39882-9a17-40e2-8181-2043d27a5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up ViZDoom environment\n",
    "def setup_vizdoom():\n",
    "    \n",
    "    game = vzd.DoomGame()\n",
    "    game.set_doom_scenario_path(os.path.join(vzd.scenarios_path, \"basic.wad\"))\n",
    "    game.set_doom_map(\"map01\")\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_160X120)\n",
    "    game.set_screen_format(vzd.ScreenFormat.RGB24)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_available_buttons([vzd.Button.MOVE_LEFT, vzd.Button.MOVE_RIGHT, vzd.Button.ATTACK])\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_living_reward(-1)\n",
    "    game.init()\n",
    "    \n",
    "    return game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27263acc-ada3-4556-9713-aaad131817a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "  def __init__(self, in_dim, out_dim):\n",
    "    super(FeedForwardNN, self).__init__()\n",
    "      \n",
    "    self.layer1 = nn.Linear(in_dim, 64)\n",
    "    self.layer2 = nn.Linear(64, 64)\n",
    "    self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "  def forward(self, obs):\n",
    "      \n",
    "    if isinstance(obs, np.ndarray):\n",
    "      obs = torch.tensor(obs, dtype=torch.float)\n",
    "  \n",
    "    activation1 = F.relu(self.layer1(obs))\n",
    "    activation2 = F.relu(self.layer2(activation1))\n",
    "    output = self.layer3(activation2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3615654-ff42-473d-8fda-2f009dbde3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_hight: int, image_width: int, num_actions: int):\n",
    "        \n",
    "        super(CNNActorCritic, self).__init__()\n",
    "\n",
    "        h = image_hight \n",
    "        w = image_width\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=4)\n",
    "        h //=4\n",
    "        w //=4\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=4)\n",
    "        h //=4\n",
    "        w //=4\n",
    "        \n",
    "        self.shared_fc = nn.Linear(h * w * 16, 128)\n",
    "        self.actor_fc = nn.Linear(128, num_actions)\n",
    "        self.critic_fc = nn.Linear(128, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        x = self.conv1 (x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.relu(self.shared_fc(x))\n",
    "        \n",
    "        actor_output = self.actor_fc(x)  \n",
    "        critic_output = self.critic_fc(x)  \n",
    "\n",
    "        return actor_output, critic_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe7c6bd-07e0-48f0-aea6-8dd5dabf7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self._init_hyperparameters()\n",
    "        self.env = env\n",
    "        \n",
    "        screen_shape = self.env.get_state().screen_buffer.shape\n",
    "        \n",
    "        self.obs_dim = np.prod(screen_shape)\n",
    "        self.act_dim = len(env.get_available_buttons()) # 3\n",
    "        \n",
    "        self.actor = CNNActorCritic(120, 160, 3)\n",
    "        self.critic = CNNActorCritic(120, 160, 1)\n",
    "\n",
    "        #self.actor = FeedForwardNN(self.obs_dim, self.act_dim)\n",
    "        #self.critic = FeedForwardNN(self.obs_dim, 1)\n",
    "\n",
    "        #self.cov_var = torch.full(size=(self.act_dim,), fill_value=0.5)\n",
    "        #self.cov_mat = torch.diag(self.cov_var)\n",
    "\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)\n",
    "        \n",
    "        self.actions = [\n",
    "            [True, False, False],  # MOVE_LEFT\n",
    "            [False, True, False],  # MOVE_RIGHT\n",
    "            [False, False, True],  # ATTACK\n",
    "        ]\n",
    "\n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch = 4800  # Number of timesteps to run per batch\n",
    "        self.max_timesteps_per_episode = 1600  # Max number of timesteps per episode\n",
    "        self.n_updates_per_iteration = 5 #Â number of epochs \n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.clip = 0.2\n",
    "        \n",
    "        self.lr = 0.0003\n",
    "        \n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        \n",
    "        t_so_far = 0  \n",
    "        \n",
    "        while t_so_far < total_timesteps: # 10000\n",
    "            \n",
    "            batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout() \n",
    "\n",
    "            t_so_far += np.sum(batch_lens)\n",
    "            \n",
    "            V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "            \n",
    "            A_k = batch_rtgs - V.detach()\n",
    "            A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "            \n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                \n",
    "                V, current_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "                ratios = torch.exp(current_log_probs - batch_log_probs) \n",
    "                \n",
    "                surr1 = ratios * A_k\n",
    "                surr2 = torch.clamp(ratios,1-self.clip, 1+self.clip) * A_k\n",
    "                \n",
    "                actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "                critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "                \n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "                self.critic_optim.zero_grad() \n",
    "                critic_loss.backward()  \n",
    "                self.critic_optim.step()\n",
    "                \n",
    "            \n",
    "    def rollout(self):\n",
    "        # Since this is an on-policy algorithm, we'll need to collect a fresh batch\n",
    "        # of data each time we iterate the actor/critic networks.\n",
    "\n",
    "        batch_obs = []  # Observations collected this batch\n",
    "        batch_acts = []  # Actions collected this batch\n",
    "        batch_log_probs = []  # Log probabilities of each action taken this batch\n",
    "        batch_rews = []  # Rewards: (number of episodes, number of timesteps per episode)\n",
    "        batch_rtgs = []  # Rewards-To-Go of each timestep in this batch\n",
    "        batch_lens = []  # Lengths of each episode this batch\n",
    "        \n",
    "        t = 0\n",
    "        while t < self.timesteps_per_batch: # 1000 \n",
    "            episode_rewards = []\n",
    "            \n",
    "            self.env.new_episode() # equivalent for reset \n",
    "\n",
    "            obs = self.env.get_state().screen_buffer\n",
    "            obs = obs.transpose(2, 0, 1)  \n",
    "            obs = obs / 255.0 \n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "            \n",
    "            for ep_t in range(self.max_timesteps_per_episode):  # 500\n",
    "                t += 1\n",
    "\n",
    "                batch_obs.append(obs)\n",
    "                \n",
    "                action_idx, log_prob = self.get_action(obs.unsqueeze(0)) \n",
    "                batch_acts.append(action_idx)\n",
    "                \n",
    "                action = self.actions[action_idx]\n",
    "                reward = self.env.make_action(action) # like a step, which retrieves a reward from an action \n",
    "                if len(episode_rewards) % 100 == 0:\n",
    "                    print(f\"Intermediate reward (last 100 timesteps): {sum(episode_rewards[-100:])}\")\n",
    "\n",
    "                \n",
    "                if self.env.is_episode_finished():\n",
    "                    break\n",
    "\n",
    "                episode_rewards.append(reward)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                \n",
    "                obs = self.env.get_state().screen_buffer\n",
    "                obs = obs.transpose(2, 0, 1)  \n",
    "                obs = obs / 255.0\n",
    "                obs = torch.tensor(obs, dtype=torch.float)\n",
    "                \n",
    "            print(\"Total reward:\", env.get_total_reward())\n",
    "\n",
    "                    \n",
    "            batch_lens.append(ep_t + 1)\n",
    "            batch_rews.append(episode_rewards)\n",
    "            \n",
    "\n",
    "\n",
    "        batch_obs = torch.stack(batch_obs)\n",
    "        #batch_obs = torch.tensor(batch_obs, dtype=torch.float)\n",
    "        #batch_acts = torch.tensor(batch_acts, dtype=torch.float)\n",
    "        if isinstance(batch_acts, torch.Tensor):\n",
    "            batch_acts = batch_acts.clone().detach().long()\n",
    "        else:\n",
    "            batch_acts = torch.as_tensor(batch_acts, dtype=torch.long)\n",
    "        batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float) \n",
    "        batch_rtgs = self.compute_rtgs(batch_rews)\n",
    "        \n",
    "        return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens \n",
    "\n",
    "\n",
    "\n",
    "    def compute_rtgs(self, batch_rews):\n",
    "        \n",
    "        # rewards-to-go per episode in the batch \n",
    "        batch_rtgs = []\n",
    "\n",
    "        for episode_reward in reversed(batch_rews):\n",
    "\n",
    "            reward_to_go = 0\n",
    "            \n",
    "            for reward in reversed(episode_reward):\n",
    "                reward_to_go = reward + reward_to_go * self.gamma\n",
    "                batch_rtgs.insert(0, reward_to_go)\n",
    "                \n",
    "        batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float)\n",
    "        \n",
    "        return (batch_rtgs - batch_rtgs.mean()) / (batch_rtgs.std() + 1e-10)\n",
    "\n",
    "\n",
    "    def evaluate(self, batch_obs, batch_acts):\n",
    "        \n",
    "        _, V = self.critic(batch_obs)\n",
    "        V = V.squeeze()  \n",
    "\n",
    "        batch_acts = torch.tensor(batch_acts, dtype=torch.long)\n",
    "\n",
    "        logits, _ = self.actor(batch_obs)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        log_prob = dist.log_prob(batch_acts)\n",
    "\n",
    "        return V, log_prob\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \n",
    "        logits, _ = self.actor(obs)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.detach().item(), log_prob.detach()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229e7b2-b1bd-450a-a82b-9a0b27f61c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available actions: [<Button.MOVE_LEFT: 11>, <Button.MOVE_RIGHT: 10>, <Button.ATTACK: 0>]\n",
      "<class 'numpy.ndarray'>\n",
      "Screen Buffer Shape: (120, 160, 3)\n",
      "Intermediate reward (last 100 timesteps): 0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Total reward: -111.0\n",
      "Intermediate reward (last 100 timesteps): 0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -115.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Total reward: -1850.0\n",
      "Intermediate reward (last 100 timesteps): 0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Total reward: -1850.0\n",
      "Intermediate reward (last 100 timesteps): 0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -130.0\n",
      "Intermediate reward (last 100 timesteps): -125.0\n",
      "Intermediate reward (last 100 timesteps): -120.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Intermediate reward (last 100 timesteps): -100.0\n",
      "Total reward: -1850.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rt/8d3k1s0n05lf125qlvnw1jx80000gn/T/ipykernel_38722/2027104368.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_acts = torch.tensor(batch_acts, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set up ViZDoom\n",
    "    env = setup_vizdoom()\n",
    "\n",
    "    print(f\"Available actions: {env.get_available_buttons()}\")\n",
    "    \n",
    "    state = env.get_state()\n",
    "    screen_buffer = state.screen_buffer  # Get the screen buffer (image)\n",
    "\n",
    "    print(type(screen_buffer))\n",
    "\n",
    "    print(f\"Screen Buffer Shape: {screen_buffer.shape}\")\n",
    "\n",
    "\n",
    "    # Initialize PPO\n",
    "    model = PPO(env)\n",
    "\n",
    "    # Train PPO\n",
    "    model.learn(10000)\n",
    "\n",
    "    # Close environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54b735-94b4-4892-8f58-e8f27c71fa96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e55bf79-8c42-4fa0-9038-cca70be0ac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8394be6-6f98-45a1-ba6e-f6cbbbb5126a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
