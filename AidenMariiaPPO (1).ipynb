{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dfdf78-932f-45de-a762-0b6223b84b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eafe4ddd-196c-450f-8a54-cbb588bbec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827635 sha256=5fe3e4c6d75241f9a05799fc010d63f54b2a89873633975c05dccf63574d52d8\n",
      "  Stored in directory: c:\\users\\19096\\appdata\\local\\pip\\cache\\wheels\\95\\51\\6c\\9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f59b7b-1fc4-44cc-860e-a2963e73958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 6.8/38.8 MB 46.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.1/38.8 MB 47.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.8 MB 52.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.8 MB 51.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 45.7 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc80a64c-1bd7-4f45-a060-a87e09f1610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "18f96b5c-7f64-4981-920a-96eca2a2da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.20 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.5.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.9.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
      "Requirement already satisfied: pygame in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.66.5)\n",
      "Requirement already satisfied: rich in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (13.7.1)\n",
      "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
      "  Downloading ale_py-0.10.1-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\19096\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from rich->stable-baselines3[extra]) (2.15.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\19096\\anaconda3\\lib\\site-packages (from tqdm->stable-baselines3[extra]) (0.4.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
      "Downloading ale_py-0.10.1-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 2.1/5.5 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.5 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading grpcio-1.68.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 0.0/4.4 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.6/4.4 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.4/4.4 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: tensorboard-data-server, grpcio, ale-py, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 ale-py-0.10.1 grpcio-1.68.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4dc03d18-cf8e-40ea-818e-76d67ea0f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\19096\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\19096\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77333010-2de0-4148-8f9d-a6593aecf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e389fd9-49be-4264-a3e2-738a0ba0c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92b6f39-758e-43db-9d04-465964fed079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410706f0-ca56-4457-9664-ac49e52c8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('github/VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        #Sets up observation space(image of game) and action space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 2)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized  = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.reshape(resized, (1, 100, 160))\n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09277eb2-78b2-439e-8fd0-b43719b96706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(model, env, trials=5):\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits, _ = agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.argmax(probs, dim=-1).item()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                total_reward += reward\n",
    "                done = terminated\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "        average_reward = np.mean(total_rewards)\n",
    "        print(f\"Average Reward over {trials} episodes: {average_reward}\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbb3a74-fbdb-4c4c-a492-17ca6d0413be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #based logic on stablebaselines\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),  # Output: [32, 24, 39]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: [64, 11, 18]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: [64, 9, 16]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(512, action_space.n)\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        x = self.forward(state)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88e0f4e-b3f7-486d-8f1b-220d7c4b64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, agent, run_name, epochs=10, clip_eps=0.2, gamma=0.99, lr=1e-4, batch_size=3000):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.clip_eps = clip_eps\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "        log_dir = os.path.join(\"ppo_logs\", run_name)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0  \n",
    "\n",
    "    def compute_returns(self, rewards, dones, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            if dones[step]:\n",
    "                R = 0  \n",
    "            R = rewards[step] + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        timesteps = 0\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0\n",
    "        episode = 0\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            states = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            dones = []\n",
    "\n",
    "            for _ in range(self.batch_size):\n",
    "                logits, value = self.agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob.detach())\n",
    "                rewards.append(reward)\n",
    "                values.append(value.detach().item())\n",
    "                dones.append(terminated)\n",
    "\n",
    "                episode_reward += reward\n",
    "                self.global_step += 1  \n",
    "\n",
    "                if terminated:\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "\n",
    "                    # Calculate and log average reward\n",
    "                    if len(episode_rewards) >= 100:\n",
    "                        average_reward = np.mean(episode_rewards[-100:])\n",
    "                    else:\n",
    "                        average_reward = np.mean(episode_rewards)\n",
    "                    self.writer.add_scalar(\"Average Reward\", average_reward, self.global_step)\n",
    "\n",
    "                    episode_reward = 0\n",
    "                    episode += 1\n",
    "                    state, _ = self.env.reset()\n",
    "                    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "                timesteps += 1\n",
    "\n",
    "            returns = self.compute_returns(rewards, dones, self.gamma)\n",
    "\n",
    "            states = torch.cat(states)\n",
    "            actions = torch.tensor(actions).unsqueeze(-1)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            returns = torch.tensor(returns).unsqueeze(-1)\n",
    "            values = torch.tensor(values).unsqueeze(-1)\n",
    "\n",
    "            advantages = returns - values\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            losses = []\n",
    "            entropies = []\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                logits, value = self.agent.get_action(states)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = (new_log_probs - log_probs).exp()\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
    "\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.SmoothL1Loss()(value, returns)\n",
    "                loss = 100*policy_loss + 0.5 * value_loss - 0.03 * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "\n",
    "            avg_policy_loss = np.mean(policy_losses)\n",
    "            avg_value_loss = np.mean(value_losses)\n",
    "            avg_entropy = np.mean(entropies)\n",
    "            avg_loss = np.mean(losses)\n",
    "\n",
    "            self.writer.add_scalar(\"Loss\", avg_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Policy Loss\", avg_policy_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Value Loss\", avg_value_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Entropy\", avg_entropy, self.global_step)\n",
    "\n",
    "\n",
    "            print(f\"Timesteps: {timesteps}, Policy Loss: {avg_policy_loss}, \"f\"Value Loss: {avg_value_loss}, Entropy: {avg_entropy}, \"f\"Combined Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3debb2-30a0-483a-9dbd-746513aa3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: -1.0\n",
      "Episode 1 Reward: -1.0\n",
      "Episode 2 Reward: 0.0\n",
      "Episode 3 Reward: 0.0\n",
      "Episode 4 Reward: -1.0\n",
      "Episode 5 Reward: -1.0\n",
      "Episode 6 Reward: 0.0\n",
      "Episode 7 Reward: 0.0\n",
      "Episode 8 Reward: 0.0\n",
      "Episode 9 Reward: -1.0\n",
      "Episode 10 Reward: 0.0\n",
      "Episode 11 Reward: -1.0\n",
      "Episode 12 Reward: 1.0\n",
      "Episode 13 Reward: 0.0\n",
      "Episode 14 Reward: 0.0\n",
      "Episode 15 Reward: -1.0\n",
      "Episode 16 Reward: 1.0\n",
      "Episode 17 Reward: 1.0\n",
      "Episode 18 Reward: 0.0\n",
      "Timesteps: 3000, Policy Loss: -0.004126956413158522, Value Loss: 0.16283128410577774, Entropy: 1.0889335870742798, Combined Loss: -0.36394800841808317\n",
      "Episode 19 Reward: 0.0\n",
      "Episode 20 Reward: 1.0\n",
      "Episode 21 Reward: 1.0\n",
      "Episode 22 Reward: 0.0\n",
      "Episode 23 Reward: 0.0\n",
      "Episode 24 Reward: -1.0\n",
      "Episode 25 Reward: 0.0\n",
      "Episode 26 Reward: 0.0\n",
      "Episode 27 Reward: 0.0\n",
      "Episode 28 Reward: 0.0\n",
      "Episode 29 Reward: -1.0\n",
      "Episode 30 Reward: -1.0\n",
      "Episode 31 Reward: 0.0\n",
      "Episode 32 Reward: 0.0\n",
      "Episode 33 Reward: 0.0\n",
      "Episode 34 Reward: 0.0\n",
      "Episode 35 Reward: 0.0\n",
      "Episode 36 Reward: 2.0\n",
      "Episode 37 Reward: 1.0\n",
      "Timesteps: 6000, Policy Loss: -2.4597320516406286e-06, Value Loss: 0.1748269334435463, Entropy: 1.087164831161499, Combined Loss: 0.05455254875123501\n",
      "Episode 38 Reward: 1.0\n",
      "Episode 39 Reward: 0.0\n",
      "Episode 40 Reward: -1.0\n",
      "Episode 41 Reward: 2.0\n",
      "Episode 42 Reward: 1.0\n",
      "Episode 43 Reward: 1.0\n",
      "Episode 44 Reward: 1.0\n",
      "Episode 45 Reward: 1.0\n",
      "Episode 46 Reward: 2.0\n",
      "Episode 47 Reward: 0.0\n",
      "Episode 48 Reward: 0.0\n",
      "Episode 49 Reward: 0.0\n",
      "Episode 50 Reward: 0.0\n",
      "Episode 51 Reward: 1.0\n",
      "Episode 52 Reward: 0.0\n",
      "Episode 53 Reward: 0.0\n",
      "Episode 54 Reward: 0.0\n",
      "Episode 55 Reward: 0.0\n",
      "Episode 56 Reward: -1.0\n",
      "Timesteps: 9000, Policy Loss: -0.002315322199549286, Value Loss: 0.20288729220628737, Entropy: 1.0813350319862365, Combined Loss: -0.1625286251306534\n",
      "Episode 57 Reward: 0.0\n",
      "Episode 58 Reward: 0.0\n",
      "Episode 59 Reward: 2.0\n",
      "Episode 60 Reward: 2.0\n",
      "Episode 61 Reward: 1.0\n",
      "Episode 62 Reward: 0.0\n",
      "Episode 63 Reward: 0.0\n",
      "Episode 64 Reward: 2.0\n",
      "Episode 65 Reward: 1.0\n",
      "Episode 66 Reward: 0.0\n",
      "Episode 67 Reward: 3.0\n",
      "Episode 68 Reward: 0.0\n",
      "Episode 69 Reward: 1.0\n",
      "Episode 70 Reward: 0.0\n",
      "Episode 71 Reward: 0.0\n",
      "Episode 72 Reward: 3.0\n",
      "Timesteps: 12000, Policy Loss: -0.002062499790265626, Value Loss: 0.3012430727481842, Entropy: 1.0645291805267334, Combined Loss: -0.08756431937217712\n",
      "Episode 73 Reward: 0.0\n",
      "Episode 74 Reward: 4.0\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "env = VizDoomGym(render=False)\n",
    "agent = PPOAgent(env.action_space)\n",
    "run_name= \"defend_9\"\n",
    "ppo = PPO(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    run_name=run_name)\n",
    "total_timesteps = 100000\n",
    "ppo.learn(total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3ed67632-b0cc-4a7c-a3d3-b009d0e589d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'ppo_vizdoom_agent_{run_name}.pth'\n",
    "torch.save(agent.state_dict(), filename)\n",
    "basic_file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "2f56a120-ca49-4749-a08f-f31e7b9286ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 2 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 3 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 4 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n",
      "Episode 5 Reward: -1.0\n",
      "Average Reward over 5 episodes: -1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_policy(agent, env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "152ca637-cf0a-48fc-8c95-f905069d66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cc575-ca90-4ca7-a077-05c2d9bc32e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
