{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88dfdf78-932f-45de-a762-0b6223b84b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: github: No such file or directory\n",
      "fatal: destination path 'ViZDoom' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!cd github & git clone https://github.com/Farama-Foundation/ViZDoom.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe4ddd-196c-450f-8a54-cbb588bbec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f59b7b-1fc4-44cc-860e-a2963e73958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80a64c-1bd7-4f45-a060-a87e09f1610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f96b5c-7f64-4981-920a-96eca2a2da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc03d18-cf8e-40ea-818e-76d67ea0f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77333010-2de0-4148-8f9d-a6593aecf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e389fd9-49be-4264-a3e2-738a0ba0c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a92b6f39-758e-43db-9d04-465964fed079",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "410706f0-ca56-4457-9664-ac49e52c8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config('VizDoom/scenarios/defend_the_center.cfg')\n",
    "\n",
    "        if render:\n",
    "            self.game.set_window_visible(True)\n",
    "        else:\n",
    "            self.game.set_window_visible(False)\n",
    "            \n",
    "        self.game.init()\n",
    "\n",
    "        #Sets up observation space(image of game) and action space\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1, 100, 160), dtype = np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 2)\n",
    "        terminated = self.game.is_episode_finished()\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        if not terminated:\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.preprocess(state)\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape, dtype=np.uint)\n",
    "        \n",
    "        return state, reward, terminated, truncated, info\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "            \n",
    "    def reset(self, seed =None, options = None):\n",
    "        if seed is not None:\n",
    "            self.game.set_seed(seed)\n",
    "\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        observation = self.preprocess(state)\n",
    "        info = {}\n",
    "        return observation, info\n",
    "        \n",
    "    #Grayscale game frame and resize it \n",
    "    def preprocess(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resized  = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_AREA)\n",
    "        state = np.reshape(resized, (1, 100, 160))\n",
    "        return state.astype(np.float32) / 255.0\n",
    "        \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09277eb2-78b2-439e-8fd0-b43719b96706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(model, env, trials=5):\n",
    "    total_rewards = []\n",
    "    for episode in range(trials):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                logits, _ = agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                action = torch.argmax(probs, dim=-1).item()\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                total_reward += reward\n",
    "                done = terminated\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1} Reward: {total_reward}\")\n",
    "        average_reward = np.mean(total_rewards)\n",
    "        print(f\"Average Reward over {trials} episodes: {average_reward}\")\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bbb3a74-fbdb-4c4c-a492-17ca6d0413be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #based logic on stablebaselines\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),  # Output: [32, 24, 39]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),  # Output: [64, 11, 18]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),  # Output: [64, 9, 16]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 9 * 16, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy_head = nn.Linear(512, action_space.n)\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        x = self.forward(state)\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f88e0f4e-b3f7-486d-8f1b-220d7c4b64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, agent, run_name, epochs=10, clip_eps=0.2, gamma=0.99, lr=1e-4, batch_size=3000):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.clip_eps = clip_eps\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "        log_dir = os.path.join(\"ppo_logs\", run_name)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.global_step = 0  \n",
    "\n",
    "    def compute_returns(self, rewards, dones, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            if dones[step]:\n",
    "                R = 0  \n",
    "            R = rewards[step] + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return returns\n",
    "\n",
    "    def learn(self, total_timesteps):\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        timesteps = 0\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0\n",
    "        episode = 0\n",
    "\n",
    "        while timesteps < total_timesteps:\n",
    "            states = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            values = []\n",
    "            dones = []\n",
    "\n",
    "            for _ in range(self.batch_size):\n",
    "                logits, value = self.agent.get_action(state)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                next_state = torch.FloatTensor(next_state.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                log_probs.append(log_prob.detach())\n",
    "                rewards.append(reward)\n",
    "                values.append(value.detach().item())\n",
    "                dones.append(terminated)\n",
    "\n",
    "                episode_reward += reward\n",
    "                self.global_step += 1  \n",
    "\n",
    "                if terminated:\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    print(f\"Episode {episode} Reward: {episode_reward}\")\n",
    "\n",
    "                    # Calculate and log average reward\n",
    "                    if len(episode_rewards) >= 100:\n",
    "                        average_reward = np.mean(episode_rewards[-100:])\n",
    "                    else:\n",
    "                        average_reward = np.mean(episode_rewards)\n",
    "                    self.writer.add_scalar(\"Average Reward\", average_reward, self.global_step)\n",
    "\n",
    "                    episode_reward = 0\n",
    "                    episode += 1\n",
    "                    state, _ = self.env.reset()\n",
    "                    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "                timesteps += 1\n",
    "\n",
    "            returns = self.compute_returns(rewards, dones, self.gamma)\n",
    "\n",
    "            states = torch.cat(states)\n",
    "            actions = torch.tensor(actions).unsqueeze(-1)\n",
    "            log_probs = torch.stack(log_probs)\n",
    "            returns = torch.tensor(returns).unsqueeze(-1)\n",
    "            values = torch.tensor(values).unsqueeze(-1)\n",
    "\n",
    "            advantages = returns - values\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            losses = []\n",
    "            entropies = []\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                logits, value = self.agent.get_action(states)\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_log_probs = dist.log_prob(actions.squeeze(-1)).unsqueeze(-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratio = (new_log_probs - log_probs).exp()\n",
    "                surr1 = ratio * advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_eps, 1 + self.clip_eps) * advantages\n",
    "\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                value_loss = nn.SmoothL1Loss()(value, returns)\n",
    "                loss = 100*policy_loss + 0.5 * value_loss - 0.03 * entropy\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropies.append(entropy.item())\n",
    "\n",
    "\n",
    "            avg_policy_loss = np.mean(policy_losses)\n",
    "            avg_value_loss = np.mean(value_losses)\n",
    "            avg_entropy = np.mean(entropies)\n",
    "            avg_loss = np.mean(losses)\n",
    "\n",
    "            self.writer.add_scalar(\"Loss\", avg_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Policy Loss\", avg_policy_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Value Loss\", avg_value_loss, self.global_step)\n",
    "            self.writer.add_scalar(\"Entropy\", avg_entropy, self.global_step)\n",
    "\n",
    "\n",
    "            print(f\"Timesteps: {timesteps}, Policy Loss: {avg_policy_loss}, \"f\"Value Loss: {avg_value_loss}, Entropy: {avg_entropy}, \"f\"Combined Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "774ebd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOCallback:\n",
    "    def __init__(self, check_freq, save_path, eval_env, eval_trials=5, verbose=1):\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_trials = eval_trials\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self, ppo_agent, timesteps):\n",
    "        if timesteps % self.check_freq == 0:\n",
    "            # Save the model\n",
    "            model_path = os.path.join(self.save_path, f'ppo_model_{timesteps}.pth')\n",
    "            torch.save(ppo_agent.agent.state_dict(), model_path)\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved at step {timesteps}\")\n",
    "\n",
    "            # Evaluate policy\n",
    "            avg_reward = evaluate_policy(ppo_agent.agent, self.eval_env, trials=self.eval_trials)\n",
    "            if self.verbose:\n",
    "                print(f\"Average Reward after {timesteps} timesteps: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c3debb2-30a0-483a-9dbd-746513aa3e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 Reward: 0.0\n",
      "Episode 1 Reward: 0.0\n",
      "Episode 2 Reward: 0.0\n",
      "Episode 3 Reward: 1.0\n",
      "Episode 4 Reward: 1.0\n",
      "Episode 5 Reward: 0.0\n",
      "Episode 6 Reward: 1.0\n",
      "Episode 7 Reward: 1.0\n",
      "Episode 8 Reward: 1.0\n",
      "Episode 9 Reward: 0.0\n",
      "Episode 10 Reward: -1.0\n",
      "Episode 11 Reward: 1.0\n",
      "Episode 12 Reward: 1.0\n",
      "Episode 13 Reward: 0.0\n",
      "Episode 14 Reward: -1.0\n",
      "Episode 15 Reward: -1.0\n",
      "Episode 16 Reward: 0.0\n",
      "Episode 17 Reward: 0.0\n",
      "Episode 18 Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[1;32m      8\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m      9\u001b[0m     agent\u001b[38;5;241m=\u001b[39magent,\n\u001b[1;32m     10\u001b[0m     run_name\u001b[38;5;241m=\u001b[39mrun_name)\n\u001b[1;32m     11\u001b[0m total_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[0;32m---> 12\u001b[0m ppo\u001b[38;5;241m.\u001b[39mlearn(total_timesteps)\n\u001b[1;32m     14\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_vizdoom_agent_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(agent\u001b[38;5;241m.\u001b[39mstate_dict(), filename)\n",
      "Cell \u001b[0;32mIn[31], line 122\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps)\u001b[0m\n\u001b[1;32m    119\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mpolicy_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m value_loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.03\u001b[39m \u001b[38;5;241m*\u001b[39m entropy\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 122\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    125\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "env = VizDoomGym(render=False)\n",
    "agent = PPOAgent(env.action_space)\n",
    "\n",
    "callback  = PPOCallback(check_freq=1000, save_path=\"ppo_checkpoints\", eval_env=env, verbose=1)\n",
    "run_name= \"defend_9\"\n",
    "ppo = PPO(\n",
    "    env=env,\n",
    "    agent=agent,\n",
    "    run_name=run_name)\n",
    "total_timesteps = 100000\n",
    "ppo.learn(total_timesteps)\n",
    "\n",
    "filename = f'ppo_vizdoom_agent_{run_name}.pth'\n",
    "torch.save(agent.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed67632-b0cc-4a7c-a3d3-b009d0e589d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f56a120-ca49-4749-a08f-f31e7b9286ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = VizDoomGym(render=False)\n",
    "callback = PPOCallback(\n",
    "        check_freq=5000, \n",
    "        save_path=\"ppo_checkpoints\", \n",
    "        eval_env=eval_env, \n",
    "        eval_trials=5, \n",
    "        verbose=1)\n",
    "\n",
    "evaluate_policy(agent, eval_env, trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ca637-cf0a-48fc-8c95-f905069d66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cc575-ca90-4ca7-a077-05c2d9bc32e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
